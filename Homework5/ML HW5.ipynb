{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPSCI 617 Machine Learning\n",
    "## Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Probabilistic Interpretation of Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bm}[1]{\\mathbf{#1}}\n",
    "l(\\beta) = ||\\bm{y} - \\bm{X}\\beta||_2^2 + \\lambda||\\beta||_2^2$\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\begin{align}\n",
    "l(\\beta) &= (\\bm{y} - \\bm{X}\\beta)^T(\\bm{y} - \\bm{X}\\beta) + \\lambda \\beta^T\\beta \\\\ \n",
    "&= \\big(\\bm{y}^T\\bm{y} - \\bm{y}^T\\bm{X}\\beta - \\beta^T \\bm{X}^T\\bm{y} + \\beta^T\\bm{X}^T\\bm{X}\\beta\\big) +\\lambda \\beta^T\\beta\n",
    "\\end{align}\n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\mathrm{d}l(\\beta)}{\\mathrm{d}\\beta} &= 0 - \\bm{y}^T\\bm{X} - \\big(\\bm{X}^T\\bm{y}\\big)^T + 2\\beta^T\\bm{X}^T\\bm{X} + 2\\lambda\\beta^T = 0 \\\\\n",
    "&= -2\\bm{y}^T\\bm{X} + 2\\beta^T\\bm{X}^T\\bm{X} + 2\\lambda\\beta^T = 0 \\\\\n",
    "&\\Rightarrow -\\bm{y}^T\\bm{X} + \\beta^T\\bm{X}^T\\bm{X} + \\lambda\\beta^T = 0 \\\\\n",
    "&\\Rightarrow \\beta^T\\big(\\bm{X}^T\\bm{X} + \\lambda\\bm{I}\\big) = \\bm{y}^T\\bm{X} \\\\\n",
    "&\\Rightarrow \\left[\\beta^T\\big(\\bm{X}^T\\bm{X} + \\lambda\\bm{I}\\big)\\right]^T = \\big(\\bm{y}^T\\bm{X}\\big)^T \\\\\n",
    "&\\Rightarrow \\big(\\bm{X}^T\\bm{X} + \\lambda\\bm{I}\\big)^T\\beta = \\bm{X}^T\\bm{y}\\\\\n",
    "&\\Rightarrow \\big(\\bm{X}^T\\bm{X} + \\lambda\\bm{I}\\big)\\beta = \\bm{X}^T\\bm{y} \\\\\n",
    "&\\Rightarrow \\beta =\\big(\\bm{X}^T\\bm{X} + \\lambda\\bm{I}\\big)^{-1} \\bm{X}^T\\bm{y}\n",
    "\\end{align} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{split}\n",
    "Pr(\\beta|\\bm{y},\\bm{X},\\lambda) &= \\cfrac{Pr(\\bm{y}|\\bm{X},\\beta) Pr(\\beta)}{Pr(\\bm{y})} \\\\\n",
    "& \\propto Pr(\\bm{y}|\\bm{X},\\beta) Pr(\\beta) \\\\\n",
    "& \\propto \\exp\\left\\{-\\frac{1}{2}(\\bm{y} - \\bm{X}\\beta)^T(\\bm{I}_{n\\times n})^{-1}(\\bm{y} - \\bm{X}\\beta)\\right\\} \\exp\\left\\{-\\frac{1}{2}(\\beta^T\\lambda \\bm{I} \\beta)\\right\\} \\\\\n",
    "& \\propto \\exp\\left\\{-\\frac{1}{2}(\\bm{y} - \\bm{X}\\beta)^T(\\bm{y} - \\bm{X}\\beta) -\\frac{1}{2}(\\beta^T \\lambda \\bm{I}\\beta)\\right\\} \\\\\n",
    "& \\propto  \\exp\\left\\{-\\frac{1}{2} \\left[(\\bm{y}^T - \\beta^T\\bm{X}^T)(\\bm{y} - \\bm{X}\\beta) + \\beta^T \\lambda \\bm{I}\\beta \\right] \\right\\} \\\\\n",
    "& \\propto  \\exp\\left\\{-\\frac{1}{2} \\left[(\\bm{y}^T\\bm{y} - \\bm{y}^T\\bm{X}\\beta - \\beta^T\\bm{X}^T\\bm{y} + \\beta^T\\bm{X}^T\\bm{X}\\beta) + \\beta^T \\lambda \\bm{I}\\beta \\right] \\right\\} \\\\\n",
    "&\\propto  \\exp\\left\\{-\\frac{1}{2}\\bm{y}^T\\bm{y}\\right\\} \\exp\\left\\{-\\frac{1}{2} \\left[- \\bm{y}^T\\bm{X}\\beta - \\beta^T\\bm{X}^T\\bm{y} + \\beta^T\\bm{X}^T\\bm{X}\\beta + \\beta^T \\lambda \\bm{I}\\beta \\right] \\right\\} \\\\\n",
    "&\\propto \\exp\\left\\{-\\frac{1}{2} \\left[- \\bm{y}^T\\bm{X}\\beta - \\beta^T\\bm{X}^T\\bm{y} + \\beta^T\\bm{X}^T\\bm{X}\\beta + \\beta^T \\lambda \\bm{I}\\beta \\right] \\right\\}      \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ (1)  \\\\\n",
    "\\end{split}\n",
    "$\n",
    "<br>\n",
    "Analysis:\n",
    "<br>\n",
    "$\n",
    "\\bm{y}^T\\bm{X}\\beta = (\\beta^T\\bm{X}^T\\bm{y})^T, \\ \n",
    "$\n",
    "$\\bm{y}$ is a $n \\times 1$ vector, $\\bm{X}$ is a $n \\times p$ matrix, $\\beta$ is a $p \\times 1$ vector;\n",
    "<br>\n",
    "$\n",
    "\\bm{y}^T\\bm{X}\\beta: (1 \\times n) (n \\times p) (p \\times 1) = 1 \\times 1 \n",
    "$\n",
    "which implies $\\bm{y}^T\\bm{X}\\beta$ is a scalar, and the transpose of a scalar is itself, \n",
    "<br>\n",
    "therefore $\\beta^T\\bm{X}^T\\bm{y} = \\bm{y}^T\\bm{X}\\beta\n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\begin{align*}\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  &\\propto \\exp\\left\\{-\\frac{1}{2} \\left[ -2\\beta^T\\bm{X}^T\\bm{y} + \\beta^T\\bm{X}^T\\bm{X}\\beta + \\beta^T \\lambda \\bm{I}\\beta \\right] \\right\\} \\\\\n",
    "&\\propto \\exp\\left\\{-\\frac{1}{2} \\left[ -2\\beta^T\\bm{X}^T\\bm{y} + \\beta^T(\\bm{X}^T\\bm{X}+\\lambda \\bm{I})\\beta \\right] \\right\\}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start from the equation $(1)$ which was derived in the middle of the proof from problem 1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Pr(\\beta|\\bm{y},\\bm{X},\\lambda) &\\propto \\exp\\left\\{-\\frac{1}{2} \\left[- \\bm{y}^T\\bm{X}\\beta - \\beta^T\\bm{X}^T\\bm{y} + \\beta^T\\bm{X}^T\\bm{X}\\beta + \\beta^T \\lambda \\bm{I}\\beta \\right] \\right\\} \\\\\n",
    "&\\propto \\exp\\left\\{-\\frac{1}{2} \\left[- \\bm{y}^T\\bm{X}\\beta - \\beta^T\\bm{X}^T\\bm{y} + \\beta^T(\\bm{X}^T\\bm{X}+\\lambda \\bm{I})\\beta \\right] \\right\\} \\\\\n",
    "& \\propto \\exp\\left\\{-\\frac{1}{2} \\left[- \\bm{y}^T\\bm{X}\\beta - \\beta^T\\bm{X}^T\\bm{y} + \\beta^T(\\bm{X}^T\\bm{X}+\\lambda \\bm{I})\\beta \\right] \\right\\} \\exp\\left\\{-\\frac{1}{2} \\left[\\bm{y}^T\\bm{X}(\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\bm{X}^T\\bm{y}\\right]\\right\\} \\\\\n",
    "& \\propto \\exp\\left\\{-\\frac{1}{2} \\left[- \\bm{y}^T\\bm{X}\\beta - \\beta^T\\bm{X}^T\\bm{y} + \\beta^T(\\bm{X}^T\\bm{X}+\\lambda \\bm{I})\\beta  + \\bm{y}^T\\bm{X}(\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\bm{X}^T\\bm{y} \\right]\\right\\}  \\\\\n",
    "& \\propto \\exp\\left\\{-\\frac{1}{2} \\left[\\beta^T(\\bm{X}^T\\bm{X}+\\lambda \\bm{I})\\beta - \\beta^T\\bm{X}^T\\bm{y} - \\bm{y}^T\\bm{X}\\beta + \\bm{y}^T\\bm{X}(\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\bm{X}^T\\bm{y} \\right]\\right\\} \\\\\n",
    "& \\propto \\exp\\left\\{-\\frac{1}{2} \\left[\\beta^T(\\bm{X}^T\\bm{X}+\\lambda \\bm{I})\\big(\\beta - (\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\bm{X}^T\\bm{y}\\big) - \\bm{y}^T\\bm{X}\\big(\\beta - (\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\bm{X}^T\\bm{y}\\big) \\right]\\right\\} \\\\\n",
    "& \\propto \\exp\\left\\{-\\frac{1}{2} \\left[\\big(\\beta^T(\\bm{X}^T\\bm{X}+\\lambda \\bm{I}) - \\bm{y}^T\\bm{X}\\big)\\big(\\beta - (\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\bm{X}^T\\bm{y}\\big) \\right]\\right\\} \\\\\n",
    "& \\propto \\exp\\left\\{-\\frac{1}{2} \\left[\\big(\\beta^T- \\bm{y}^T\\bm{X}(\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\big)\\big(\\bm{X}^T\\bm{X}+\\lambda \\bm{I}\\big) \\big(\\beta - (\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\bm{X}^T\\bm{y}\\big) \\right]\\right\\} \\\\\n",
    "& \\propto \\exp\\left\\{-\\frac{1}{2} \\left[\\big(\\beta- \\left[(\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\right]^T\\bm{X}^T\\bm{y}\\big)^T\\big(\\bm{X}^T\\bm{X}+\\lambda \\bm{I}\\big) \\big(\\beta - (\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\bm{X}^T\\bm{y}\\big) \\right]\\right\\} \\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "<br>\n",
    "$\\bm{X}^T\\bm{X}$ is a symmetric matrix because $\\big(\\bm{X}^T\\bm{X}\\big)^T =\\bm{X}^T\\bm{X}  $\n",
    "<br>\n",
    "$\\lambda \\bm{I}$ is also a symmetric matrix because $\\bm{I}$ is an identity matrix \n",
    "<br>\n",
    "The sum of two symmetric matrices is also a symmetric matrix;\n",
    "<br>\n",
    "Thus we have the following:\n",
    "$$\n",
    "\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^{-1} = \\left[\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^{-1}\\right]^T = \\bm{I} \n",
    "$$\n",
    "$$\n",
    "\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^{-1} = \\left[\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^{-1}\\right]^T \\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^T\n",
    "$$\n",
    "Since we have $\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^{-1} = \\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^{-1}\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)$ and $\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^T  = \\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)$:\n",
    "$$\n",
    "\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^{-1}\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)= \\left[\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^{-1}\\right]^T \\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)\n",
    "$$\n",
    "Multiply both sides by $\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^{-1}$ we have:\n",
    "$$\n",
    "\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^{-1} = \\left[\\big(\\bm{X}^T\\bm{X}+ \\lambda\\bm{I}\\big)^{-1}\\right]^T \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{split}\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ & \\propto \\exp\\left\\{-\\frac{1}{2} \\left[\\big(\\beta- (\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\bm{X}^T\\bm{y}\\big)^T\\big(\\bm{X}^T\\bm{X}+\\lambda \\bm{I}\\big) \\big(\\beta - (\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\bm{X}^T\\bm{y}\\big) \\right]\\right\\} \\\\\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is multivariate normal distribution with $\\bm{\\mu} = (\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\bm{X}^T\\bm{y}$ ;  $\\bm{\\Sigma} = (\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}$ ;\n",
    "<br>\n",
    "The normal distribution reaches its maximum when $\\beta$ equal the mean of the distribution:\n",
    "<br>\n",
    "$$\n",
    "\\beta^* = (\\bm{X}^T\\bm{X}+\\lambda \\bm{I})^{-1}\\bm{X}^T\\bm{y}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Learning Theory: Sub-Gaussian Bounds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Pr_{X\\sim\\mathcal{D}}(X > t) = \\Pr(e^{sX} > e^{st}) \\leq \\cfrac{\\mathbb{E}[e^{sX}]}{e^{st}} =\\cfrac{ e^{\\large\\frac{\\sigma^2s^2}{2}}}{e^{\\large st}} = e^{\\large\\frac{\\sigma^2s^2}{2} - st}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the tightest possible bound, we need make $\\begin{equation}e^{\\large\\frac{\\sigma^2s^2}{2} - st}\\end{equation}$ smallest possible, which means we shoule make $\\frac{\\sigma^2 s^2}{2}  -st $\n",
    "<br>\n",
    "smallest possible \n",
    "since $e^x$ is the monotonic increaseing function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phi(s) = \\frac{\\sigma^2 s^2}{2} -st \\\\ \n",
    "\\phi^{'}(s) = {\\sigma^2 s} - t =0 \\\\ \n",
    "s^* = \\frac{t}{\\sigma^2}  \\\\\n",
    "\\phi{(s^*)} = \\frac{t^2}{2\\sigma^2} - \\frac{t^2}{\\sigma^2} = - \\frac{t^2}{2\\sigma^2} \\\\\n",
    "\\Pr_{X\\sim\\mathcal{D}}(X > t) \\leq e^{ \\large- \\frac{t^2}{2\\sigma^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "<br>\n",
    "Given\n",
    "$\n",
    "X_1,...,X_n$  are n independent random variables and the functions of r.v $(e^ {\\large sX_i})$ are also independent, therefore:\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\mathbb{E}[e^{sX_1}e^{sX_2} ... e^{sX_n}] =\\mathbb{E}[e^{X_1}]\\mathbb{E}[e^{X_2}] ... \\mathbb{E}[e^{X_n}]\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\mathbb{E}[e^{sX_i}] \\leq  e^{\\large \\frac{\\sigma^2s^2}{2}} \\\\\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\mathbb{E}[e^{X_1}]\\mathbb{E}[e^{X_2}] ... \\mathbb{E}[e^{X_n}] \\leq e^{\\large \\frac{\\sigma^2s^2}{2}}e^{\\large \\frac{\\sigma^2s^2}{2}}...e^{\\large \\frac{\\sigma^2s^2}{2}}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\mathbb{E}{[e^{s\\sum_i^nX_i}]} = \\mathbb{E}[e^{sX_1}e^{sX_2} ... e^{sX_n}] =\\mathbb{E}[e^{X_1}]\\mathbb{E}[e^{X_2}] ... \\mathbb{E}[e^{X_n}] \\leq e^{\\large \\frac{\\sigma^2s^2}{2}}e^{\\large \\frac{\\sigma^2s^2}{2}}...e^{\\large \\frac{\\sigma^2s^2}{2}} \\leq e^{\\large \\frac{n\\sigma^2s^2}{2}}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\mathbb{E}{[e^{s\\sum_i^nX_i}]}\\leq e^{\\large \\frac{n\\sigma^2s^2}{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Pr(\\frac{1}{n}\\sum_{i=1}^nX_i > t) = \\Pr(\\sum_{i=1}^nX_i > nt) = \\Pr(e^{s\\sum_i X_i} > e^{snt}) \\leq \\cfrac{\\mathbb{E}[e^{s\\sum_i^n X_i}]}{e^{nst}} \\leq \\cfrac{ e^{\\large\\frac{n\\sigma^2s^2}{2}}}{e^{\\large nst}} = e^{\\large\\frac{n\\sigma^2s^2}{2} - nst}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phi(s) = \\frac{\\sigma^2ns^2}{2} -snt \\\\ \n",
    "\\phi^{'}(s) = {\\sigma^2n s} - nt =0 \\\\\n",
    "s^* = \\frac{t}{\\sigma^2}  \\\\\n",
    "\\phi{(s^*)} = \\frac{nt^2}{2\\sigma^2} - \\frac{nt^2}{\\sigma^2} = - \\frac{nt^2}{2\\sigma^2} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Pr(\\frac{1}{n}\\sum_{i=1}^nX_i > t) \\leq e^{\\large \\phi(s^*)} \\leq e^{\\large -\\frac{nt^2}{2\\sigma^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 K-Means as an Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a \n",
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective function:  $\\displaystyle \\sum_i \\text{min}_{k}||x_i - c_k||_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b\n",
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(\\tilde{\\bm{x}}^{(t)}) =\\displaystyle  \\sum_i (x_i -\\tilde{x}^{(t)}_k)^2$ for $x_i$ belongs to $C_k$\n",
    "<br>\n",
    "for the first step, we assign the the $x$ to the cloest centroid $\\tilde{x}^{(t)}$ which makes the objective:\n",
    "<br>\n",
    "$$\n",
    "L(\\tilde{\\bm{x}}^{(t)}) =\\displaystyle  \\sum_i \\text{min}_k (x_i -\\tilde{x}^{(t)}_k)^2\n",
    "$$\n",
    "<br>\n",
    "thus we have:\n",
    "$$\n",
    "\\sum_i \\text{min}_k (x_i -\\tilde{x}^{(t)}_k)^2 \\leq \\displaystyle  \\sum_i (x_i -\\tilde{x}^{(t)}_k)^2\n",
    "$$\n",
    "<br>\n",
    "Assign the new centroid based on the mean of the new clusters, which is $\\tilde{\\bm{x}}^{(t+1)}_k$\n",
    "for each cluster we have $x_i$ for $x_i \\in C_k$, we want find: \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\text{arg min} \\sum_i ||x_i - z||^2\n",
    "$$\n",
    "<br>\n",
    "Take the derivative and set it to 0, we can get:\n",
    "$$ \n",
    "2\\sum_i^n x_i - z = 0  \\\\\n",
    "z= \\frac{\\sum_i x_i}{n} = \\bar{x}  \\\\ \n",
    "$$\n",
    "thus we can conclude the distance between the mean of a cluster of data points and every single data point in that cluster is smallest, \n",
    "<br>\n",
    "since  $\\tilde{\\bm{x}}^{(t+1)}_k$ is the center of the new clusters formed in t+1 iteration, we have:\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\sum_i (x_i -\\tilde{x}^{(t+1)}_k)^2 \\leq \\sum_i \\text{min}_k(x_i -\\tilde{x}^{(t)}_k)^2\\leq \\displaystyle  \\sum_i (x_i -\\tilde{x}^{(t)}_k)^2\n",
    "$$\n",
    "<br>\n",
    "Given the proof shown above, it is clear now our objective function is monotonic decreasing, and since the function is also non-negative,which means it is bounded, therefore, K-means will converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set: suppose we have four one-dimensional data points: $x_1 = 1, x_2=2 ,x_3 = 5, x_4 = 7$. and k = 3 is asked.\n",
    "<br>\n",
    "(dataset credited to Discussion8 question 2.2)\n",
    "<br>\n",
    "<br>\n",
    "Global optimal solution: cluster 1 {$x_1, x_2$} with center $z_1 = 1.5$; cluser 2 {$x_3$} with center $z_2 = 5$; cluster 3 {$x_4$} with center $z_3 =7$\n",
    "<br>\n",
    "Initialize centers using $z_1 = x_1, z_2= x_2, z_3= x_3$, therefore we have clusters: \n",
    "$$\n",
    "\\{x_1\\};\\{x_2\\};\\{x_3,x_4\\}\n",
    "$$\n",
    "then we find out the new centers for each cluster:\n",
    "$$\n",
    "z_1^{(1)} = x_1 = 1; z_2^{(1)} = x_2 =2; z_3^{(1)} = \\frac{x_3 + x_4}{2} = 6\n",
    "$$\n",
    "if we assign each point to the new centers we would still get the same clusters because x_3 and x_4 still sit cloest to the new cluster center $z_3=6$ , \n",
    "<br>\n",
    "so it has no further update, thus we can never reach the optimal solution by this initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K means initialize the starting centorids randomly and then in the first step, and secondly gather the points to form clusters based on their distance between those centorids, lastly, k means will find out their new centers. Step 2 and 3 are repeated until convergence. In the very first step, we locally choose centarin amount of points to be the centers and then we choose $x_i$ which $\\min \\sum_i (x_i - C_k)^2$, so what we are doing is to locally optimize the cost function, and in the later step, we are going to find the centers in the new clusters, to be specifically, $\\min \\sum_i||x-z||^2$, which we are assume the other points are fixed and we only consider the minimization in the one cluster in those steps. Therefore, the algorithm cannot gauarantee global optimality. \n",
    "<br>\n",
    "<br>\n",
    "For Gussian Mixture, the objective is to maximize the observed data log-likelihood, which is $\\displaystyle \\sum_i \\log \\sum_K p(X_i=x_i | z_i=k, \\theta)p(z_i=k|\\theta)$, however EM tends to maximizes the auxiliary function which is $\\displaystyle \\sum_i \\sum_k p(z_i=k|x_i,\\theta_t) \\log \\frac{p(X_i = x_i, z_i = k |\\theta)}{p(z_i=k|x_i,\\theta_t)}$. The auxiliary function is obtained by the Jensen's Inequality which indicates log-likelihood$(\\theta) \\geq$ A$(\\theta,\\theta_t)$, therefore, EM cannot guarantee Gaussian Mixture Model always find the globally solution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 EM Algorithm for Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a\n",
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\log \\left[Pr(D=d_1, D=d_2,..D=d_i, W=w_1,W =w_2,...W=w_i  | \\alpha, \\beta)\\right] &=\\log\\left[ \\displaystyle  \\prod_i^N \\prod_j^W \\big(\\sum_k  \n",
    "Pr(d_i,z_k,w_j)\\big)^{n(w_j,d_i)}\\right] \\\\\n",
    "&=\\log \\left[ \\displaystyle  \\prod_i^N \\prod_j^W \\big(\\sum_k Pr(d_i)Pr(z_k|d_i)Pr(w_j|z_k)\\big)^{n(w_j,d_i)}\\right] \\\\\n",
    "&=\\log\\left[ \\displaystyle  \\prod_i^N \\prod_j^W \\big(\\sum_k \\frac{1}{M} \\alpha_{ik} \\beta_{kj}\\big)^{n(w_j,d_i)}\\right] \\\\\n",
    "& = \\displaystyle \\sum_i^M \\sum_j^W n(w_j,d_i)\\log\\left[ \\frac{1}{M}\\displaystyle \\sum_k  \\alpha_{ik} \\beta_{kj}\\right]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Pr(z_k|w_j,d_i,\\alpha^{old},\\beta^{old}) = \\cfrac{Pr(d_i,z_k,w_j | \\alpha^{old} \\beta^{old})}{Pr(d_i,w_j|\\alpha^{old} \\beta^{old})} = \\cfrac{\\frac{1}{M} \\alpha^{old} \\beta^{old}}{\\sum_k \\frac{1}{M}\\alpha_{ik}^{old}\\beta_{kj}^{old}} = \\cfrac{\\alpha_{ik}^{old} \\beta_{kj}^{old}}{\\displaystyle \\sum_k \\alpha_{ik}^{old}\\beta_{kj}^{old}}$\n",
    "<br>\n",
    "Let: \n",
    "<br>\n",
    "$$\n",
    "\\gamma_{ijk} = \\cfrac{\\alpha_{ik}^{old} \\beta_{kj}^{old}}{\\displaystyle \\sum_k \\alpha_{ik}^{old}\\beta_{kj}^{old}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\theta = (\\alpha , \\beta )\\\\\n",
    "\\theta_t = (\\alpha_t, \\beta_t) \\\\\n",
    "\\\\\n",
    "A(\\theta,\\theta_t)  = \\sum_i \\sum_j n(d_i,w_j)\\sum_k \\gamma_{ijk} \\log\\cfrac{Pr(d_i,z_k,w_j)}{\\gamma_{ijk}}\n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "L(\\alpha_{ik}, \\beta_{kj}) = A(\\theta,\\theta_t) + \\lambda_1(1-\\displaystyle \\Sigma_k \\alpha_{ik}) + \\lambda_2 \\displaystyle \\sum_j(1- \\beta_{kj}) \n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\begin{split}\n",
    "\\cfrac{\\partial L(\\theta, \\theta_t)}{\\partial \\alpha_{i^{'}k^{'}} } &= \\cfrac{\\partial A(\\theta, \\theta_t)}{\\partial \\alpha_{i^{'}k^{'}} } -\\lambda_1 \\\\\n",
    "&= n(w_j,d_i)\\sum_j\\frac{\\gamma_{i^{'}jk^{'}}}{\\alpha_{i^{'}k^{'}}} - \\lambda_1  \\\\\n",
    "&=0\n",
    "\\end{split}\n",
    "$\n",
    "<br>\n",
    "$\n",
    "\\alpha_{i'k'} = \\cfrac{ n(w_j,d_i)\\sum_j\\gamma_{i'jk'}}{\\lambda_1}\n",
    "$\n",
    "<br>\n",
    "Given:\n",
    "$ \n",
    "\\displaystyle \\sum_k \\alpha_{ik} = 1   \\\\\n",
    "\\displaystyle \\sum_k \\cfrac{n(w_j,d_i) \\sum_j\\gamma_{ijk}}{\\lambda_1} = 1 \\\\\n",
    "\\displaystyle \\sum_k \\sum_j n(w_j,d_i)\\gamma_{ijk}={\\lambda_1}  \\\\\n",
    "\\displaystyle \\sum_j n(w_j,d_i) \\sum_k  Pr(z_k|w_j,d_i,\\alpha^{old},\\beta^{old}) ={\\lambda_1}  \\\\\n",
    "\\displaystyle \\sum_j n(w_j,d_i)\\times1 = \\lambda_1 \\\\\n",
    "\\lambda_1 = n(d_i) \n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\alpha_{i'k'} = \\cfrac{ \\displaystyle \\sum_j^N n(w_j,d_i)\\gamma_{i'jk'}}{n(d_i)}\n",
    "$\n",
    "<br>\n",
    "$\n",
    "\\begin{split}\n",
    "\\cfrac{\\partial L(\\theta, \\theta_t)}{\\partial \\beta_{k^{'}j^{'}} } &= \\cfrac{\\partial A(\\theta, \\theta_t)}{\\partial \\alpha_{k^{'}j^{'}} } -\\lambda_2 \\\\\n",
    "&= \\sum_in(w_j,d_i)\\frac{\\gamma_{ij'k^{'}}}{\\beta_{k'j'}} - \\lambda_2  \\\\\n",
    "&=0\n",
    "\\end{split}\n",
    "$\n",
    "<br>\n",
    "$\n",
    "\\beta_{k'j'} = \\sum_i n(w_j,d_i)\\cfrac{\\gamma_{ij'k^{'}}}{\\lambda_2}\n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "Given:\n",
    "$ \n",
    "\\displaystyle \\sum_j \\beta_{kj} = 1   \\\\\n",
    "\\displaystyle \\sum_j \\cfrac{ \\sum_i n(w_j,d_i)\\gamma_{ijk}}{\\lambda_2} = 1 \\\\\n",
    "\\displaystyle \\sum_j \\sum_i n(w_j,d_i)\\gamma_{ijk}={\\lambda_2}  \\\\\n",
    "\\beta_{k'j'} =  \\displaystyle\\cfrac{\\sum_i n(w_j,d_i)\\gamma_{ij'k'}}{\\sum_j \\sum_in(w_j,d_i)\\gamma_{ijk}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What Happens to My Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " define:\n",
    " $\n",
    "\\mathbf{z}_l = \\mathbf{W}_l^T\\mathbf{h}_{l-1} + \\mathbf{b_l} \\\\\n",
    " $\n",
    " <br>\n",
    " $\n",
    "1. \\nabla_{\\mathbf{w}_3} \\mathcal{L}(\\bm\\theta) =\\nabla_{\\mathbf{z}_3}\\mathcal{L}(\\theta)  \\cfrac{\\partial z_3}{\\partial \\mathbf{w}_3} $ \n",
    "<br>\n",
    "<br>\n",
    "where we have:\n",
    "<br>\n",
    "$\n",
    "\\cfrac{\\partial \\mathcal{L}(\\theta)}{\\partial f(\\mathbf{x_i})} = -\\displaystyle \\sum_{i=1}^N \\cfrac{y_i}{f(\\mathbf{x}_i)} + \\cfrac{y_i - 1}{1- f{(\\mathbf{x}_i})}\n",
    "$\n",
    "<br>\n",
    "$\n",
    "\\cfrac{\\partial f(\\mathbf{x_i})}{\\partial z_3} = \\sigma(z_3)(1-\\sigma(z_3))$\n",
    "<br>\n",
    "$\n",
    "\\nabla_{\\mathbf{z}_3}\\mathcal{L}(\\theta) = \\cfrac{\\partial \\mathcal{L}(\\theta)}{\\partial f(\\mathbf{x_i})}\\cfrac{\\partial f(\\mathbf{x_i})}{\\partial z_3} = -\\displaystyle \\sum_{i=1}^N  \\big[\\cfrac{y_i}{f(\\mathbf{x}_i)} + \\cfrac{y_i - 1}{1- f{(\\mathbf{x}_i})} \\big]\\sigma(z_3)(1-\\sigma(z_3))\n",
    "$\n",
    "<br>\n",
    "$\n",
    "\\cfrac{\\partial z_3}{\\partial \\mathbf{w}_3} = \\mathbf{h}_2^\\mathrm{T}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "2. \\nabla_{b_3} \\mathcal{L}(\\bm\\theta) =\\nabla_{\\mathbf{z}_3}\\mathcal{L}(\\theta) \\cfrac{\\partial z_3}{\\partial b_3} $ \n",
    "<br>\n",
    "where we have:\n",
    "<br>\n",
    "$\n",
    "\\cfrac{\\partial \\mathcal{L}(\\theta)}{\\partial f(\\mathbf{x_i})} = -\\displaystyle \\sum_{i=1}^N \\cfrac{y_i}{f(\\mathbf{x}_i)} + \\cfrac{y_i - 1}{1- f{(\\mathbf{x}_i})}\n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\cfrac{\\partial z_3}{\\partial \\mathbf{b}_3} = 1\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.$\\nabla_{\\bf{W}_2} \\mathcal{L}(\\theta) =\\nabla_{\\mathbf{z}_2}\\mathcal{L}(\\theta) \\cfrac{\\partial \\mathbf{z}_2}{\\partial \\mathbf{W}_2}$\n",
    "<br>\n",
    "where we have:\n",
    "<br>\n",
    "$\\nabla_{\\bf{z}_2} \\mathcal{L}(\\theta) =\\nabla_{\\bf{z}_3} \\mathcal{L}(\\theta)\\cfrac{\\partial \\mathbf{z}_3}{\\partial \\mathbf{z}_2}$\n",
    "<br>\n",
    "$\n",
    "\\cfrac{\\partial \\mathbf{z}_3}{\\partial \\mathbf{z}_2} = \\mathbf{w}_3\\cfrac{d\\sigma(\\mathbf{z}_2)}{d\\mathbf{z}_2} = \\mathbf{w}_3 \\circ \\sigma{(\\mathbf{z}_2})\\circ(\\bf{1}-\\sigma{(\\mathbf{z}_2})) \n",
    "$\n",
    "<br>\n",
    "Note: let $\\circ$ to be the element-wise product of two vectors. \n",
    "<br>\n",
    "<br>\n",
    "$\\nabla_{\\bf{z}_2} \\mathcal{L}(\\theta) =\\nabla_{\\bf{z}_3} \\mathcal{L}(\\theta) \\circ \\mathbf{w}_3 \\circ \\sigma{(\\mathbf{z}_2})\\circ(\\bf{1}-\\sigma{(\\mathbf{z}_2}))   $\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\cfrac{\\partial \\mathbf{z}_2}{\\partial \\mathbf{W}_2} = \\begin{bmatrix} \\\\ \\; \\; \\; \\mathbf{h}_1^T \\; \\;\\\\  \\\\\\end{bmatrix}\n",
    "$\n",
    ": this is a matrix constructed by $H$ rows of $\\mathbf{h}_1^\\mathrm{T}$\n",
    "<br>\n",
    "$\\nabla_{\\bf{W}_2} \\mathcal{L}(\\theta) =\\nabla_{\\mathbf{z}_3}\\mathcal{L}(\\theta_3) \\circ\\mathbf{w}_3 \\circ \\sigma{(\\mathbf{z}_2})\\circ(\\bf{1}-\\sigma{(\\mathbf{z}_2})) \\circ \\begin{bmatrix} \\\\ \\; \\; \\; \\mathbf{h}_1^T \\; \\;\\\\  \\\\\\end{bmatrix}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.$\\nabla_{\\bf{b}_2} \\mathcal{L}(\\theta) =\\nabla_{\\mathbf{z}_2}\\mathcal{L}(\\theta) \\cfrac{\\partial \\mathbf{z}_2}{\\partial \\mathbf{b}_2}$\n",
    "<br>\n",
    "where we have:\n",
    "<br>\n",
    "$\n",
    "\\cfrac{\\partial \\mathbf{z}_2}{\\partial \\mathbf{b}_2} = \\mathbf{I}\n",
    "$\n",
    "<br>\n",
    "Note: $\\mathbf{I}$ is a $H \\times H$ identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.$\\nabla_{\\bf{W}_1} \\mathcal{L}(\\theta) =\\nabla_{\\mathbf{z}_1}\\mathcal{L}(\\theta) \\cfrac{\\partial \\mathbf{z}_1}{\\partial \\mathbf{W}_1}$\n",
    "<br>\n",
    "where we have:\n",
    "<br>\n",
    "$\\nabla_{\\bf{z}_1} \\mathcal{L}(\\theta) =\\nabla_{\\bf{z}_2} \\mathcal{L}(\\theta)\\cfrac{\\partial \\mathbf{z}_2}{\\partial \\mathbf{z}_1}$\n",
    "<br>\n",
    "$\n",
    "\\cfrac{\\partial \\mathbf{z}_2}{\\partial \\mathbf{z}_1} = \\mathbf{W}_2\n",
    "\\cfrac{d\\sigma(\\mathbf{z}_1)}{d\\mathbf{z}_1} = \\mathbf{W}_2 \\circ \\sigma{(\\mathbf{z}_1})\\circ(\\bf{1}-\\sigma{(\\mathbf{z}_1})) \n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\nabla_{\\bf{z}_1} \\mathcal{L}(\\theta) =\\nabla_{\\bf{z}_2} \\mathcal{L}(\\theta)\\mathbf{W}_2 \\circ \\sigma{(\\mathbf{z}_1})\\circ(\\bf{1}-\\sigma{(\\mathbf{z}_1})) \n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "$\n",
    "\\cfrac{\\partial \\mathbf{z}_1}{\\partial \\mathbf{W}_1} = \\begin{bmatrix} \\\\ \\; \\; \\; \\mathbf{x}_i^T \\; \\;\\\\  \\\\\\end{bmatrix}\n",
    "$\n",
    ":this is a matrix constructed by $H$ rows of $\\mathbf{x}_i$\n",
    "$\n",
    "\\nabla_{\\bf{W}_1} \\mathcal{L}(\\theta) =\\nabla_{\\mathbf{z}_2}\\mathcal{L}(\\theta_3) \\mathbf{w}_2 \\circ \\sigma{(\\mathbf{z}_1})\\circ(\\bf{1}-\\sigma{(\\mathbf{z}_1})) \\circ \\begin{bmatrix} \\\\ \\; \\; \\; \\mathbf{x}_i^T \\; \\;\\\\  \\\\\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.$\\nabla_{\\bf{b}_1} \\mathcal{L}(\\theta) =\\nabla_{\\mathbf{z}_1}\\mathcal{L}(\\theta) \\cfrac{\\partial \\mathbf{z}_1}{\\partial \\mathbf{b}_1}$\n",
    "<br>\n",
    "where we have:\n",
    "<br>\n",
    "$\n",
    "\\cfrac{\\partial \\mathbf{z}_1}{\\partial \\mathbf{b}_1} = \\mathbf{I}\n",
    "$\n",
    "<br>\n",
    "Note: $\\mathbf{I}$ is a $H \\times H$ identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from scipy.io import loadmat\n",
    "All_data = loadmat('mnist_all.mat')\n",
    "train0  =All_data['train0']\n",
    "test0 = All_data['test0']\n",
    "train1 = All_data['train1']\n",
    "test1 = All_data['test1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.utils import resample\n",
    "class NN:\n",
    "    def __init__(self,d):\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        self.d = d\n",
    "        for i in range(d):\n",
    "            if i == 0:\n",
    "                w = 0.001*np.random.randn(4,784)\n",
    "            else:\n",
    "                w = 0.01*np.random.randn(4,4)\n",
    "            b = 0.01*np.random.randn(4)\n",
    "            self.W.append(w)\n",
    "            self.b.append(b)\n",
    "        self.W.append(0.01*np.random.randn(4))\n",
    "        self.b.append(0.01*np.random.randn())\n",
    "        \n",
    "    def activation(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    def forward(self,x):\n",
    "        h = x\n",
    "        All_h = []\n",
    "        for i in range(len(self.W)):\n",
    "            h =  self.W[i]@h + self.b[i] \n",
    "            if i !=len(self.W)-1:\n",
    "                for k, p in enumerate(h):\n",
    "                    h[k]= self.activation(p)\n",
    "            else:\n",
    "                h =self.activation(h)    \n",
    "            All_h.append(h)\n",
    "        return All_h\n",
    "    \n",
    "    def loss(self,X):\n",
    "        loss = 0\n",
    "        for i in X:\n",
    "            training_features = i[:-1]\n",
    "            y = i[-1]\n",
    "            predict = self.forward(training_features)[-1]\n",
    "            loss = loss - (y*np.log(predict) + (1-y)*np.log(1-predict))\n",
    "        print(\"loss is \", loss)\n",
    "        return loss\n",
    "    \n",
    "    def backpropagate(self,mini_batch):\n",
    "        D_w = []\n",
    "        D_b = []\n",
    "        for i in range(len(self.W)):\n",
    "            d_w = np.zeros_like(self.W[i])\n",
    "            D_w.append(d_w)\n",
    "            d_b = np.zeros_like(self.b[i])\n",
    "            D_b.append(d_b)\n",
    "        for training_sample in mini_batch:\n",
    "            layer_output = self.forward(training_sample[:-1])\n",
    "            n =  len(layer_output)\n",
    "            y = training_sample[-1]\n",
    "            gradient =[]\n",
    "            gradient_zd =-(y/layer_output[n-1] +(y-1)/(1-layer_output[n-1]))*layer_output[n-1]*(1-layer_output[n-1]) \n",
    "            gradient.append(gradient_zd)\n",
    "            for i in range(self.d,1,-1):\n",
    "                if i == self.d:\n",
    "                    gradient_z = gradient_zd*self.W[i] * layer_output[i-1]*(1-layer_output[i-1])\n",
    "                else:\n",
    "                    gradient_z = np.dot(gradient_zd,self.W[i]) * layer_output[i-1]*(1-layer_output[i-1])\n",
    "                gradient.append(gradient_z)\n",
    "                gradient_zd = gradient_z\n",
    "            gradient.append(np.dot(gradient_zd,self.W[1])* layer_output[0]*(1-layer_output[0]))\n",
    "                    \n",
    "            d_w_l = []\n",
    "            d_b_l = []\n",
    "            d_wd = gradient[0] * layer_output[self.d-1]\n",
    "            d_w_l.append(d_wd) \n",
    "\n",
    "            for i in range(self.d+1):\n",
    "                d_b_l.append(gradient[i])       ##[b_d+1, b_d .... b_1]\n",
    "            for i in range(1,len(gradient)):    \n",
    "                if i < len(gradient) - 1:\n",
    "                    d_w_i = np.array([layer_output[i-1],]*4)\n",
    "                    for j in range(gradient[i].shape[0]):\n",
    "                        d_w_i[j] = gradient[i][j] * d_w_i[j]\n",
    "                else:\n",
    "                    d_w_i = np.array([training_sample[:-1],]*4)\n",
    "                    for k in range(gradient[i].shape[0]):\n",
    "                        d_w_i[k] =  gradient[i][k]*d_w_i[k]\n",
    "#                                 print(\"gradient z1 element\", gradient[i][k])\n",
    "                d_w_l.append(d_w_i)\n",
    "                \n",
    "            for i in range(len(d_w_l)):\n",
    "                D_w[i] = D_w[i]+d_w_l[-(i+1)]\n",
    "                D_b[i] = D_b[i]+d_b_l[-(i+1)]\n",
    "        return D_w,D_b\n",
    "                \n",
    "    \n",
    "    def train(self,X):\n",
    "            i= 0\n",
    "            loss = 1000\n",
    "            l = 0\n",
    "            while loss > 300:\n",
    "                print(\"train iteration:\",l+1)\n",
    "                mini_batch = resample(X,n_samples = 200)\n",
    "                D_w, D_b= self.backpropagate(mini_batch)\n",
    "                for i in range(len(self.W)):\n",
    "                    self.W[i] = self.W[i] - 0.1 *D_w[i]/200\n",
    "                    self.b[i] = self.b[i] -  0.1 *D_b[i]/200\n",
    "                loss = self.loss(X)\n",
    "                l = l+1\n",
    "                \n",
    "    def prediction(self,X):\n",
    "        y = []\n",
    "        for x in X:\n",
    "            all_h = self.forward(x)\n",
    "            if all_h[2] > 0.5:\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "        return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1 = np.ones((train1.shape[0],1))\n",
    "label_0 = np.zeros((train0.shape[0],1))\n",
    "train1_withlabel = np.hstack((train1,label_1))\n",
    "train0_withlabel = np.hstack((train0,label_0))\n",
    "mixed_train = np.vstack((train1_withlabel,train0_withlabel))\n",
    "np.random.shuffle(mixed_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train iteration: 1\n",
      "loss is  8752.43442903357\n",
      "train iteration: 2\n",
      "loss is  8751.675910214275\n",
      "train iteration: 3\n",
      "loss is  8779.547305636424\n",
      "train iteration: 4\n",
      "loss is  8754.644557241958\n",
      "train iteration: 5\n",
      "loss is  8748.674221975412\n",
      "train iteration: 6\n",
      "loss is  8750.0424323202\n",
      "train iteration: 7\n",
      "loss is  8748.512447547539\n",
      "train iteration: 8\n",
      "loss is  8742.51841954908\n",
      "train iteration: 9\n",
      "loss is  8740.128056191892\n",
      "train iteration: 10\n",
      "loss is  8742.222995757389\n",
      "train iteration: 11\n",
      "loss is  8743.588815388623\n",
      "train iteration: 12\n",
      "loss is  8732.751839254379\n",
      "train iteration: 13\n",
      "loss is  8736.021461106897\n",
      "train iteration: 14\n",
      "loss is  8733.970800069497\n",
      "train iteration: 15\n",
      "loss is  8746.068824765403\n",
      "train iteration: 16\n",
      "loss is  8722.11788853739\n",
      "train iteration: 17\n",
      "loss is  8706.324211347563\n",
      "train iteration: 18\n",
      "loss is  8730.634748043754\n",
      "train iteration: 19\n",
      "loss is  8690.417831539296\n",
      "train iteration: 20\n",
      "loss is  8684.524195043952\n",
      "train iteration: 21\n",
      "loss is  8713.588647219885\n",
      "train iteration: 22\n",
      "loss is  8707.502702518626\n",
      "train iteration: 23\n",
      "loss is  8691.273774995054\n",
      "train iteration: 24\n",
      "loss is  8720.339567214702\n",
      "train iteration: 25\n",
      "loss is  8715.239273321393\n",
      "train iteration: 26\n",
      "loss is  8714.170177028851\n",
      "train iteration: 27\n",
      "loss is  8707.704221229855\n",
      "train iteration: 28\n",
      "loss is  8712.753584422728\n",
      "train iteration: 29\n",
      "loss is  8714.503772385297\n",
      "train iteration: 30\n",
      "loss is  8658.977337028235\n",
      "train iteration: 31\n",
      "loss is  8647.845745487391\n",
      "train iteration: 32\n",
      "loss is  8623.620870657527\n",
      "train iteration: 33\n",
      "loss is  8603.56934701011\n",
      "train iteration: 34\n",
      "loss is  8585.137784093688\n",
      "train iteration: 35\n",
      "loss is  8569.85998460793\n",
      "train iteration: 36\n",
      "loss is  8550.278231190412\n",
      "train iteration: 37\n",
      "loss is  8522.918793639297\n",
      "train iteration: 38\n",
      "loss is  8491.949352591477\n",
      "train iteration: 39\n",
      "loss is  8463.555093940358\n",
      "train iteration: 40\n",
      "loss is  8451.719929714018\n",
      "train iteration: 41\n",
      "loss is  8616.177696545865\n",
      "train iteration: 42\n",
      "loss is  8595.648647956756\n",
      "train iteration: 43\n",
      "loss is  8539.287793025822\n",
      "train iteration: 44\n",
      "loss is  8638.01315823132\n",
      "train iteration: 45\n",
      "loss is  8515.634437982657\n",
      "train iteration: 46\n",
      "loss is  8442.332570083\n",
      "train iteration: 47\n",
      "loss is  8400.597022011752\n",
      "train iteration: 48\n",
      "loss is  8353.290598817806\n",
      "train iteration: 49\n",
      "loss is  8307.126500669876\n",
      "train iteration: 50\n",
      "loss is  8253.865006888807\n",
      "train iteration: 51\n",
      "loss is  8194.919036258132\n",
      "train iteration: 52\n",
      "loss is  8120.3153670192105\n",
      "train iteration: 53\n",
      "loss is  8074.469447616121\n",
      "train iteration: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yiteng/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is  8424.911304986897\n",
      "train iteration: 55\n",
      "loss is  8391.257719945377\n",
      "train iteration: 56\n",
      "loss is  8346.90186254779\n",
      "train iteration: 57\n",
      "loss is  8303.745067309696\n",
      "train iteration: 58\n",
      "loss is  8247.888920497828\n",
      "train iteration: 59\n",
      "loss is  8196.061086224587\n",
      "train iteration: 60\n",
      "loss is  8155.760900510707\n",
      "train iteration: 61\n",
      "loss is  8077.905470040495\n",
      "train iteration: 62\n",
      "loss is  8026.26640744681\n",
      "train iteration: 63\n",
      "loss is  7941.331530077487\n",
      "train iteration: 64\n",
      "loss is  7899.041589141971\n",
      "train iteration: 65\n",
      "loss is  8290.378484239654\n",
      "train iteration: 66\n",
      "loss is  8879.814144548307\n",
      "train iteration: 67\n",
      "loss is  8694.23279676878\n",
      "train iteration: 68\n",
      "loss is  7960.924626673091\n",
      "train iteration: 69\n",
      "loss is  7890.149109410732\n",
      "train iteration: 70\n",
      "loss is  7806.390508151361\n",
      "train iteration: 71\n",
      "loss is  7609.542412954672\n",
      "train iteration: 72\n",
      "loss is  7546.837793005135\n",
      "train iteration: 73\n",
      "loss is  7148.562193462097\n",
      "train iteration: 74\n",
      "loss is  6960.913579957029\n",
      "train iteration: 75\n",
      "loss is  6794.6596425106345\n",
      "train iteration: 76\n",
      "loss is  6547.131273225412\n",
      "train iteration: 77\n",
      "loss is  6341.738412834214\n",
      "train iteration: 78\n",
      "loss is  6114.001119895446\n",
      "train iteration: 79\n",
      "loss is  5911.105644530853\n",
      "train iteration: 80\n",
      "loss is  5701.537881923788\n",
      "train iteration: 81\n",
      "loss is  5492.351630564838\n",
      "train iteration: 82\n",
      "loss is  5280.795154451597\n",
      "train iteration: 83\n",
      "loss is  5073.644353736525\n",
      "train iteration: 84\n",
      "loss is  4873.307994381389\n",
      "train iteration: 85\n",
      "loss is  4668.88649247612\n",
      "train iteration: 86\n",
      "loss is  4479.26344576228\n",
      "train iteration: 87\n",
      "loss is  4293.866495554329\n",
      "train iteration: 88\n",
      "loss is  4114.146871633474\n",
      "train iteration: 89\n",
      "loss is  3985.939839259127\n",
      "train iteration: 90\n",
      "loss is  3802.430019273324\n",
      "train iteration: 91\n",
      "loss is  3646.8159293918484\n",
      "train iteration: 92\n",
      "loss is  3858.4370570093183\n",
      "train iteration: 93\n",
      "loss is  3384.482241189223\n",
      "train iteration: 94\n",
      "loss is  3260.1890151194148\n",
      "train iteration: 95\n",
      "loss is  3129.7413874141694\n",
      "train iteration: 96\n",
      "loss is  3005.303300552003\n",
      "train iteration: 97\n",
      "loss is  2794.499860720523\n",
      "train iteration: 98\n",
      "loss is  2681.7896344381784\n",
      "train iteration: 99\n",
      "loss is  2572.125282129917\n",
      "train iteration: 100\n",
      "loss is  2476.2761721280403\n",
      "train iteration: 101\n",
      "loss is  2381.4665881950195\n",
      "train iteration: 102\n",
      "loss is  2293.8575597610165\n",
      "train iteration: 103\n",
      "loss is  2212.5408659393415\n",
      "train iteration: 104\n",
      "loss is  2130.6958696037377\n",
      "train iteration: 105\n",
      "loss is  2053.9207825491567\n",
      "train iteration: 106\n",
      "loss is  1999.3953003767074\n",
      "train iteration: 107\n",
      "loss is  1932.7694760522681\n",
      "train iteration: 108\n",
      "loss is  1872.3998926019294\n",
      "train iteration: 109\n",
      "loss is  1812.5233972879087\n",
      "train iteration: 110\n",
      "loss is  1760.0581189540019\n",
      "train iteration: 111\n",
      "loss is  1708.5642950822687\n",
      "train iteration: 112\n",
      "loss is  1660.745504891076\n",
      "train iteration: 113\n",
      "loss is  1607.5824937985367\n",
      "train iteration: 114\n",
      "loss is  1562.7349846519842\n",
      "train iteration: 115\n",
      "loss is  1521.2774271428761\n",
      "train iteration: 116\n",
      "loss is  1479.9843749962874\n",
      "train iteration: 117\n",
      "loss is  1438.5978313549924\n",
      "train iteration: 118\n",
      "loss is  1404.704012025987\n",
      "train iteration: 119\n",
      "loss is  1371.3172699657016\n",
      "train iteration: 120\n",
      "loss is  1339.554895420973\n",
      "train iteration: 121\n",
      "loss is  1308.896557286555\n",
      "train iteration: 122\n",
      "loss is  7095.730816015849\n",
      "train iteration: 123\n",
      "loss is  3711.3368101624505\n",
      "train iteration: 124\n",
      "loss is  3050.8913925061497\n",
      "train iteration: 125\n",
      "loss is  2856.151416987241\n",
      "train iteration: 126\n",
      "loss is  2740.6157486537\n",
      "train iteration: 127\n",
      "loss is  2639.840084694524\n",
      "train iteration: 128\n",
      "loss is  2550.6763602006167\n",
      "train iteration: 129\n",
      "loss is  2440.9798936631646\n",
      "train iteration: 130\n",
      "loss is  2350.476521189716\n",
      "train iteration: 131\n",
      "loss is  2269.1924923512724\n",
      "train iteration: 132\n",
      "loss is  2189.654225564148\n",
      "train iteration: 133\n",
      "loss is  2124.5323586373947\n",
      "train iteration: 134\n",
      "loss is  2056.3278196208507\n",
      "train iteration: 135\n",
      "loss is  1992.0242993331367\n",
      "train iteration: 136\n",
      "loss is  1873.5773310652946\n",
      "train iteration: 137\n",
      "loss is  1819.1224574762155\n",
      "train iteration: 138\n",
      "loss is  1767.500350379105\n",
      "train iteration: 139\n",
      "loss is  1710.7198872174795\n",
      "train iteration: 140\n",
      "loss is  7215.698302589191\n",
      "train iteration: 141\n",
      "loss is  6560.593676669497\n",
      "train iteration: 142\n",
      "loss is  6059.511166217396\n",
      "train iteration: 143\n",
      "loss is  5519.143576991916\n",
      "train iteration: 144\n",
      "loss is  5438.590781043724\n",
      "train iteration: 145\n",
      "loss is  5423.7069061916045\n",
      "train iteration: 146\n",
      "loss is  5338.143272483981\n",
      "train iteration: 147\n",
      "loss is  10702.1463463207\n",
      "train iteration: 148\n",
      "loss is  1877.7165602574144\n",
      "train iteration: 149\n",
      "loss is  1832.973675360301\n",
      "train iteration: 150\n",
      "loss is  1794.3950484293975\n",
      "train iteration: 151\n",
      "loss is  1755.008368789455\n",
      "train iteration: 152\n",
      "loss is  1717.7205420977125\n",
      "train iteration: 153\n",
      "loss is  1685.470932656552\n",
      "train iteration: 154\n",
      "loss is  1640.0196179366217\n",
      "train iteration: 155\n",
      "loss is  1609.249524751639\n",
      "train iteration: 156\n",
      "loss is  1577.7330458414058\n",
      "train iteration: 157\n",
      "loss is  1541.01023568148\n",
      "train iteration: 158\n",
      "loss is  1501.646530529339\n",
      "train iteration: 159\n",
      "loss is  1475.850110620305\n",
      "train iteration: 160\n",
      "loss is  1422.8690238412912\n",
      "train iteration: 161\n",
      "loss is  1393.7480743712033\n",
      "train iteration: 162\n",
      "loss is  1375.118654129303\n",
      "train iteration: 163\n",
      "loss is  1356.603950636398\n",
      "train iteration: 164\n",
      "loss is  1337.4494063803966\n",
      "train iteration: 165\n",
      "loss is  1322.818174037471\n",
      "train iteration: 166\n",
      "loss is  1307.9250138913699\n",
      "train iteration: 167\n",
      "loss is  1289.7131189201275\n",
      "train iteration: 168\n",
      "loss is  1271.374013050505\n",
      "train iteration: 169\n",
      "loss is  1225.4448529704403\n",
      "train iteration: 170\n",
      "loss is  1204.769025733853\n",
      "train iteration: 171\n",
      "loss is  1168.5339415159206\n",
      "train iteration: 172\n",
      "loss is  1148.887296949434\n",
      "train iteration: 173\n",
      "loss is  1113.7251215675406\n",
      "train iteration: 174\n",
      "loss is  1054.7337674019352\n",
      "train iteration: 175\n",
      "loss is  1045.1968343460412\n",
      "train iteration: 176\n",
      "loss is  1002.7151727040435\n",
      "train iteration: 177\n",
      "loss is  998.475324069252\n",
      "train iteration: 178\n",
      "loss is  988.9279178770588\n",
      "train iteration: 179\n",
      "loss is  978.4678722576316\n",
      "train iteration: 180\n",
      "loss is  968.5112760570673\n",
      "train iteration: 181\n",
      "loss is  962.9292495368776\n",
      "train iteration: 182\n",
      "loss is  955.6131987273815\n",
      "train iteration: 183\n",
      "loss is  946.7027494416369\n",
      "train iteration: 184\n",
      "loss is  942.4111231796381\n",
      "train iteration: 185\n",
      "loss is  938.1974572846322\n",
      "train iteration: 186\n",
      "loss is  918.152950442541\n",
      "train iteration: 187\n",
      "loss is  911.8189898875514\n",
      "train iteration: 188\n",
      "loss is  907.4375839817568\n",
      "train iteration: 189\n",
      "loss is  886.1257294755748\n",
      "train iteration: 190\n",
      "loss is  882.0798451918716\n",
      "train iteration: 191\n",
      "loss is  879.0430810800322\n",
      "train iteration: 192\n",
      "loss is  874.9803309212716\n",
      "train iteration: 193\n",
      "loss is  870.4412823200195\n",
      "train iteration: 194\n",
      "loss is  867.0462638740373\n",
      "train iteration: 195\n",
      "loss is  862.8639989695724\n",
      "train iteration: 196\n",
      "loss is  861.2164881543001\n",
      "train iteration: 197\n",
      "loss is  856.5478085831147\n",
      "train iteration: 198\n",
      "loss is  843.9853078235769\n",
      "train iteration: 199\n",
      "loss is  842.3765999600517\n",
      "train iteration: 200\n",
      "loss is  838.6953849367745\n",
      "train iteration: 201\n",
      "loss is  834.4908628858083\n",
      "train iteration: 202\n",
      "loss is  831.8754797633765\n",
      "train iteration: 203\n",
      "loss is  831.2120704866027\n",
      "train iteration: 204\n",
      "loss is  829.8658748870304\n",
      "train iteration: 205\n",
      "loss is  826.6164126111812\n",
      "train iteration: 206\n",
      "loss is  824.5514391852829\n",
      "train iteration: 207\n",
      "loss is  822.7159418444618\n",
      "train iteration: 208\n",
      "loss is  819.6855580037702\n",
      "train iteration: 209\n",
      "loss is  817.6409132602334\n",
      "train iteration: 210\n",
      "loss is  815.9364122113261\n",
      "train iteration: 211\n",
      "loss is  777.9854017665564\n",
      "train iteration: 212\n",
      "loss is  775.5193022468068\n",
      "train iteration: 213\n",
      "loss is  774.3638340823055\n",
      "train iteration: 214\n",
      "loss is  751.0705732912742\n",
      "train iteration: 215\n",
      "loss is  749.9599554714932\n",
      "train iteration: 216\n",
      "loss is  747.6343522948981\n",
      "train iteration: 217\n",
      "loss is  744.9153425547222\n",
      "train iteration: 218\n",
      "loss is  742.9758858853155\n",
      "train iteration: 219\n",
      "loss is  727.538901570851\n",
      "train iteration: 220\n",
      "loss is  727.2911557065144\n",
      "train iteration: 221\n",
      "loss is  725.3936789821927\n",
      "train iteration: 222\n",
      "loss is  724.7244131226405\n",
      "train iteration: 223\n",
      "loss is  722.9900034988314\n",
      "train iteration: 224\n",
      "loss is  720.9671669905881\n",
      "train iteration: 225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is  718.6895548894776\n",
      "train iteration: 226\n",
      "loss is  717.1616524848619\n",
      "train iteration: 227\n",
      "loss is  716.172442650025\n",
      "train iteration: 228\n",
      "loss is  716.1247923865532\n",
      "train iteration: 229\n",
      "loss is  715.3950030558103\n",
      "train iteration: 230\n",
      "loss is  713.6365304844282\n",
      "train iteration: 231\n",
      "loss is  712.6419263290213\n",
      "train iteration: 232\n",
      "loss is  684.745912544982\n",
      "train iteration: 233\n",
      "loss is  684.3316822750762\n",
      "train iteration: 234\n",
      "loss is  682.9125934084811\n",
      "train iteration: 235\n",
      "loss is  683.2802717574863\n",
      "train iteration: 236\n",
      "loss is  681.4225254312024\n",
      "train iteration: 237\n",
      "loss is  680.5559184926299\n",
      "train iteration: 238\n",
      "loss is  678.8477259502858\n",
      "train iteration: 239\n",
      "loss is  678.2605635191512\n",
      "train iteration: 240\n",
      "loss is  676.9532436113687\n",
      "train iteration: 241\n",
      "loss is  675.3972275202443\n",
      "train iteration: 242\n",
      "loss is  670.7534041880372\n",
      "train iteration: 243\n",
      "loss is  670.4570097730123\n",
      "train iteration: 244\n",
      "loss is  669.2210927859425\n",
      "train iteration: 245\n",
      "loss is  668.9750496999329\n",
      "train iteration: 246\n",
      "loss is  668.5783196488698\n",
      "train iteration: 247\n",
      "loss is  669.4783626356772\n",
      "train iteration: 248\n",
      "loss is  668.8054926065514\n",
      "train iteration: 249\n",
      "loss is  664.9303729603699\n",
      "train iteration: 250\n",
      "loss is  663.8046914237764\n",
      "train iteration: 251\n",
      "loss is  663.517862774264\n",
      "train iteration: 252\n",
      "loss is  662.9458573883787\n",
      "train iteration: 253\n",
      "loss is  661.0670383925601\n",
      "train iteration: 254\n",
      "loss is  659.5684135638961\n",
      "train iteration: 255\n",
      "loss is  657.6478746943401\n",
      "train iteration: 256\n",
      "loss is  656.339242859339\n",
      "train iteration: 257\n",
      "loss is  655.3178784635729\n",
      "train iteration: 258\n",
      "loss is  654.2861529573665\n",
      "train iteration: 259\n",
      "loss is  653.7559552335094\n",
      "train iteration: 260\n",
      "loss is  653.7626887292498\n",
      "train iteration: 261\n",
      "loss is  626.8937473852415\n",
      "train iteration: 262\n",
      "loss is  625.9244328622335\n",
      "train iteration: 263\n",
      "loss is  625.047586485777\n",
      "train iteration: 264\n",
      "loss is  624.7925961123517\n",
      "train iteration: 265\n",
      "loss is  623.5889505109785\n",
      "train iteration: 266\n",
      "loss is  622.6756796495649\n",
      "train iteration: 267\n",
      "loss is  621.7078371649197\n",
      "train iteration: 268\n",
      "loss is  621.395161761568\n",
      "train iteration: 269\n",
      "loss is  620.5917673736602\n",
      "train iteration: 270\n",
      "loss is  455.2118511130284\n",
      "train iteration: 271\n",
      "loss is  452.64858490548625\n",
      "train iteration: 272\n",
      "loss is  450.96763537758596\n",
      "train iteration: 273\n",
      "loss is  358.8630904135262\n",
      "train iteration: 274\n",
      "loss is  355.7664128710326\n",
      "train iteration: 275\n",
      "loss is  353.20280031730545\n",
      "train iteration: 276\n",
      "loss is  348.48012017042964\n",
      "train iteration: 277\n",
      "loss is  345.794536410531\n",
      "train iteration: 278\n",
      "loss is  342.92009558546664\n",
      "train iteration: 279\n",
      "loss is  334.4796512821134\n",
      "train iteration: 280\n",
      "loss is  332.6630695560398\n",
      "train iteration: 281\n",
      "loss is  329.9861944414054\n",
      "train iteration: 282\n",
      "loss is  328.58059152670637\n",
      "train iteration: 283\n",
      "loss is  326.9003166204607\n",
      "train iteration: 284\n",
      "loss is  324.9795295590014\n",
      "train iteration: 285\n",
      "loss is  324.21073314107366\n",
      "train iteration: 286\n",
      "loss is  322.2855530646597\n",
      "train iteration: 287\n",
      "loss is  319.5563958552086\n",
      "train iteration: 288\n",
      "loss is  317.6043109656069\n",
      "train iteration: 289\n",
      "loss is  315.5846032285294\n",
      "train iteration: 290\n",
      "loss is  314.69537344348703\n",
      "train iteration: 291\n",
      "loss is  313.239160718777\n",
      "train iteration: 292\n",
      "loss is  312.6956856592236\n",
      "train iteration: 293\n",
      "loss is  311.46448325397336\n",
      "train iteration: 294\n",
      "loss is  310.2602517373299\n",
      "train iteration: 295\n",
      "loss is  308.829206871365\n",
      "train iteration: 296\n",
      "loss is  307.3635677138115\n",
      "train iteration: 297\n",
      "loss is  306.3701042312\n",
      "train iteration: 298\n",
      "loss is  305.8480029384701\n",
      "train iteration: 299\n",
      "loss is  304.5894578515217\n",
      "train iteration: 300\n",
      "loss is  304.8542784921216\n",
      "train iteration: 301\n",
      "loss is  303.581023058542\n",
      "train iteration: 302\n",
      "loss is  303.00429687481966\n",
      "train iteration: 303\n",
      "loss is  301.7406206147364\n",
      "train iteration: 304\n",
      "loss is  300.2124006374216\n",
      "train iteration: 305\n",
      "loss is  298.976425560085\n"
     ]
    }
   ],
   "source": [
    "c=NN(2)\n",
    "c.train(mixed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yiteng/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy on digit 0 is 0.996938775510204\n",
      "The training accuracy on digit 0 is 0.9969609994934999\n",
      "The test accuracy on digit 1 is  1.0\n",
      "The training accuracy on digit 1 is  0.9970335212103233\n"
     ]
    }
   ],
   "source": [
    "P0 = c.prediction(test0)\n",
    "P1 = c.prediction(test1)\n",
    "p_train0 = c.prediction(train0)\n",
    "p_train1 =c.prediction(train1)\n",
    "accuracy_0 = sum(P0 - np.zeros(test0.shape[0]))/test0.shape[0]\n",
    "print(\"The test accuracy on digit 0 is\", 1-accuracy_0)\n",
    "accuracy_0 = sum(p_train0 - np.zeros(train0.shape[0]))/train0.shape[0]\n",
    "print(\"The training accuracy on digit 0 is\", 1-accuracy_0)\n",
    "accuracy_1 = sum(np.ones(test1.shape[0]) - P1)/test1.shape[0] \n",
    "print(\"The test accuracy on digit 1 is \", 1- accuracy_1)\n",
    "accuracy_1 = sum(np.ones(train1.shape[0]) - p_train1)/train1.shape[0] \n",
    "print(\"The training accuracy on digit 1 is \", 1- accuracy_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "### d =  1\n",
    "mini_batch = resample(mixed_train,n_samples=200)\n",
    "c1 = NN(1)\n",
    "g_w1,_ = c1.backpropagate(mini_batch)\n",
    "c2 = NN(2)\n",
    "g_w2,_=c2.backpropagate(mini_batch)\n",
    "c3 = NN(3)\n",
    "g_w3,_=c3.backpropagate(mini_batch)\n",
    "c4 = NN(4)\n",
    "g_w4,_=c4.backpropagate(mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-layer w_1 gradient: 148.64171167643602\n",
      "2-layer w_1 gradient: 0.904329701884857\n",
      "3-layer w_1 gradient: 0.005497313162780188\n",
      "4-layer w_1 gradient: 0.00011249048769731869\n"
     ]
    }
   ],
   "source": [
    "print(\"1-layer w_1 gradient:\",np.linalg.norm(g_w1[0]))\n",
    "print(\"2-layer w_1 gradient:\",np.linalg.norm(g_w2[0]))\n",
    "print(\"3-layer w_1 gradient:\",np.linalg.norm(g_w3[0]))\n",
    "print(\"4-layer w_1 gradient:\",np.linalg.norm(g_w4[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\nabla \\mathbf{W}_1 \\mathcal{L}(\\theta) =  \\cfrac{\\partial \\mathcal{L}{(\\theta)}}{\\partial f{(x)}}\\cfrac{\\partial f(x)}{\\partial z_{l}}\\cfrac{\\partial z_{l}}{\\partial z_{l-1}}\\cfrac{\\partial z_{l-1}}{\\partial z_{l-2}} ... \\cfrac{\\partial z_{1}}{\\partial \\mathbf{w}_{1}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have: \n",
    "        $$\n",
    "        \\cfrac{\\partial z_l}{\\partial z_{l-1}} =\\mathbf{W}_l \\circ \\sigma{(\\mathbf{z}_l})\\circ(\\mathbf{1}-\\sigma{(\\mathbf{z}_l})) \n",
    "        $$\n",
    "Analysis:\n",
    "<br>\n",
    "<br>\n",
    "Let $\\Sigma$ = $ \\sigma{(\\mathbf{z}_l})\\circ(\\mathbf{1}-\\sigma{(\\mathbf{z}_l})) $ is Hadamard product which the result is a vector in which element is $\\sigma{(z_{li}})(1-\\sigma{(z_{li}}))$\n",
    "<br>\n",
    "Now, we take few steps to see the upper bound of the sigmoid derivative:  \n",
    "$$\\sigma{(z})(1-\\sigma{(z}))  = \\cfrac{e^{-x}}{(1+e^{-x})^2}$$\n",
    "<br>\n",
    "Take the derivative of $\\sigma{(z})(1-\\sigma{(z}))$ and set it to 0 , we can get:\n",
    "$$\n",
    "\\cfrac{e^{x}(e^{x}-1)}{(1+e^x)^3} = 0 \\\\\n",
    "e^x = 0 \\;  e^{x} = 1\n",
    "$$\n",
    "<br>\n",
    "$e^x = 1$ is the upper bound for the sigmoid derivative, therefore, we have $\\sigma{(z})(1-\\sigma{(z}))  \\leq \\frac{1}{4}$, which means every element in that vector \n",
    "<br>\n",
    "is bounded by $\\frac{1}{4}$\n",
    "<br>\n",
    "<br>\n",
    "Back to $\\mathbf{W}_l \\circ \\Sigma$, this equation shows that every row of $\\mathbf{W}_l$ would multiply by the element in $\\Sigma$, which essential means \n",
    "<br>\n",
    "$\\bf{W}$ is multiplying a diagonal matrix in which every entry on the diagonal is an element in $\\Sigma$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\cfrac{\\partial z_l}{\\partial z_{l-1}} =\\mathbf{W}_l \\cdot \\text{diag}(\\Sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one of the basic matrix norm inequality we have:\n",
    "$$\n",
    "\\bigg|\\bigg|\\cfrac{\\partial z_l}{\\partial z_{l-1}}\\bigg|\\bigg| =\\bigg|\\bigg|\\mathbf{W}_l \\cdot \\text{diag}(\\Sigma)\\bigg|\\bigg|\\leq \\big|\\big|\\mathbf{W}_l\\big|\\big| \\ \\big|\\big| \\text{diag}(\\Sigma)\\big|\\big|\n",
    "$$\n",
    "<br>\n",
    "Given the norm of diagonal matrix is bounded by the maximum entry on the diagonal, so it is bounded by $\\frac{1}{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the $W$ are diagonalizable, so they can all be written in the eigenvector decomposition form. $W = P\\Lambda P^{-1}$ We can also have the $W^T = (P^{-1})^T \\Lambda P^T$, here we define P as the matrix constructed by the eigenvectors with orthnormal basis. Therefore, according to the definition of L2 norm for the matrix: \n",
    "$$\n",
    "||A||_2 = \\sqrt{\\lambda_{max}(A^TA)} = \\sqrt{\\lambda_{max}(P\\Lambda^2P^T)} = \\sigma_{max}(A)\n",
    "$$\n",
    "<br>\n",
    "The eigenvalues are embedded in $\\Lambda$,the maximum singular value is the absolute value of the max eigenvalue. Therefore $||A||_2$ is bounded by $4 - \\epsilon$\n",
    "<br>\n",
    "so we have:\n",
    "$$\n",
    "\\bigg|\\bigg|\\cfrac{\\partial z_l}{\\partial z_{l-1}}\\bigg|\\bigg|  \\leq \\frac{1}{4} * (4-\\epsilon) \\leq 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{split}\n",
    "\\bigg|\\bigg|\\nabla \\mathbf{W}_1 \\mathcal{L}(\\theta)\\bigg|\\bigg|_{d -> \\infty} &= \\bigg|\\bigg| \\cfrac{\\partial \\mathcal{L}{(\\theta)}}{\\partial f{(x)}}\\cfrac{\\partial f(x)}{\\partial z_{l}}\\cfrac{\\partial z_{l}}{\\partial z_{l-1}}\\cfrac{\\partial z_{l-1}}{\\partial z_{l-2}} ... \\cfrac{\\partial z_{1}}{\\partial \\mathbf{w}_{1}} \\bigg|\\bigg| \\\\\n",
    "&\\leq \\bigg|\\bigg| \\cfrac{\\partial \\mathcal{L}{(\\theta)}}{\\partial f{(x)}}\\bigg|\\bigg| \\bigg|\\bigg| \\cfrac{\\partial f(x)}{\\partial z_{l}}\\bigg|\\bigg|\\bigg|\\bigg|  \\cfrac{\\partial z_{l}}{\\partial z_{l-1}}\\bigg|\\bigg| \\bigg|\\bigg| \\cfrac{\\partial z_{l-1}}{\\partial z_{l-2}} \\bigg|\\bigg| ... \\bigg|\\bigg| \\cfrac{\\partial z_{1}}{\\partial \\mathbf{w}_{1}} \\bigg|\\bigg| \n",
    "\\end{split}\n",
    "$ \n",
    "<br>\n",
    "if d goes into infinity, \n",
    "then there are infinite numeber of layers, which means infinite numbers\n",
    "less than 1 would be \n",
    "multiplied together, \n",
    "<br>\n",
    "then the overall result goes to 0; if the norm of $\\nabla \\mathbf{W}_1 \\mathcal{L}(\\theta)$ is zero, we can conclude that:\n",
    "$$\\lim_{d\\to\\infty}\\nabla_{\\mathbf{W}_1} \\mathcal{L}(\\bm\\theta)=\\mathbf{0}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
