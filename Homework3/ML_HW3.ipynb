{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probit Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 3.5>\n",
    "Given $Y = \\begin{cases} 1 &\\mbox{if } Z > 0\\\\ 0 &\\mbox{otherwise} \\end{cases}$ and $Z = \\bf{x}\\beta + \\epsilon \\ \\ $,  then $\\mathrm{Pr}(Y=1|\\mathbf{x}) = \\mathrm{Pr}(Z > 0) = \\mathrm{Pr}(Z = \\bf{x}\\beta + \\epsilon > \\mathrm{0})$ \n",
    "<br>\n",
    "<br>\n",
    "$\\Rightarrow  \\mathrm{Pr}(\\bf{x}\\beta + \\epsilon > \\mathrm{0}) =\\mathrm{Pr}(\\epsilon > -\\bf{x}\\beta) $  \n",
    "<br>\n",
    "<br>\n",
    "Because $\\epsilon$ belongs to a standard nomral $\\mathcal{N}(0, 1)$, $\\mathrm{Pr}(\\epsilon > -\\bf{x}\\beta) = \\mathrm{1} - \\mathrm{Pr}(\\epsilon <-\\bf{x}\\beta)  = \\mathrm{1} - \\mathrm{\\Phi(-\\bf{x}\\beta)} =\\mathrm{\\Phi(\\bf{x}\\beta)}$\n",
    "<br>\n",
    "<br>\n",
    "Therefore: $\\Pr(Y=1|\\bf{x}) = \\mathrm{\\Phi(\\bf{x}\\beta)}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 3.5>\n",
    "$Y \\sim Bernoulli(p), p = \\Phi(\\bf{x}\\beta)ï¼š \\\\ $\n",
    "<br>\n",
    "$f(y|\\bf{x},\\beta) $ $=$ ${\\Phi(\\bf{x}\\beta)}^y{(1-\\Phi(\\bf{x}\\beta))}^{1-y}$, for $y=0,1$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$\n",
    "L(\\beta) = \\mathrm{Pr}(Y=y_1... Y=y_n | \\beta, x_1...x_n) = \\displaystyle \\prod_{i=1}^N {\\Phi(\\bf{x_i}\\beta)}^{y_i}{(1-\\Phi(\\bf{x_i}\\beta))}^{1-y_i}  \\\\\n",
    "\\begin{align}\n",
    "logL(\\beta) &= log \\displaystyle \\prod_{i=1}^N {\\Phi(\\bf{x_i}\\beta)}^{y_i}{(1-\\Phi(\\bf{x_i}\\beta))}^{1-y_i}  \\\\\n",
    "            &= \\displaystyle \\sum_{i=1}^N log {\\Phi(\\bf{x_i}\\beta)}^{y_i}{(1-\\Phi(\\bf{x_i}\\beta))}^{1-y_i} \\\\\n",
    "            &=\\displaystyle \\sum_{i=1}^N log {\\Phi(\\bf{x_i}\\beta)}^{y_i} + log{(1-\\Phi(\\bf{x_i}\\beta))}^{1-y_i}\\\\\n",
    "            &=\\displaystyle \\sum_{i=1}^N log {\\Phi(\\bf{x_i}\\beta)}^{y_i} + log{(\\Phi(-\\bf{x_i}\\beta))}^{1-y_i} \\\\\n",
    "            &=\\displaystyle \\sum_{i=1}^N y_i\\  log {\\Phi(\\bf{x_i}\\beta)} + {(1-y_i)} \\ log{(\\Phi(-\\bf{x_i}\\beta))}\n",
    "\\end{align}\n",
    "$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4.5>\n",
    "$\n",
    "\\frac{\\partial -\\log\\Phi(a)}{\\partial a} = -\\frac{\\phi(a)}{\\Phi{a}}$\n",
    "<br>\n",
    "<br>\n",
    "$\\frac{\\partial^2-\\log(\\Phi(a))}{\\partial a^2} =\\frac{\\partial -\\frac{\\phi(a)}{\\Phi{a}}}{\\partial a} =-\\frac{-a\\phi{(a)}\\Phi{(a)}-(\\phi{(a)})^2}{(\\Phi(a))^2} =\\frac{a\\phi(a)\\Phi{(a)}+(\\phi{(a)})^2}{(\\Phi(a))^2} \n",
    "$\n",
    "</font>    \n",
    "<br>\n",
    "<br>\n",
    "Given $a\\Phi(a) + \\phi(a) > 0\\;\\text{and } \\phi(a) >0 \\;\\forall a $:\n",
    "<br>\n",
    "$$\n",
    "\\\\\n",
    "\\\\\n",
    "a\\phi{(a)}\\Phi{(a)}+(\\phi{(a)})^2 = \\phi{(a)}[a\\Phi{(a)} + \\phi{(a)}] \\geq 0\n",
    "$$\n",
    "<br>\n",
    "The denominator is ($\\Phi{(x)})^2$ , so $\\frac{\\partial^2-\\log(\\Phi(a))}{\\partial a^2} = \\frac{a\\phi{(a)}\\Phi{(a)}+(\\phi{(a)})^2}{(\\Phi(a))^2} \\geq 0 $ which indicates that $\\log\\Phi{(a)}$ is convex for all $a$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial -\\log \\mathcal{L}(\\beta)}{\\partial \\beta} = -\\sum_{i=1}^N y_i\\frac{x_i\\phi({\\mathbf{x}_i}\\beta)}{\\Phi(\\mathbf{x}_i\\beta)} + (1-y_i)\\frac{-x_i\\phi{(\\mathbf{x}_i\\beta)}}{1- \\Phi(\\mathbf{x}_i\\beta)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adaboost with $L_2$ Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LHS:\n",
    "<br>\n",
    "$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\begin{align}\n",
    "\\argmax\\limits_{j=1,\\dots,m} \\left[-\\frac{\\partial}{\\partial \\alpha}L({\\mathbf{\\lambda}} + \\alpha\\mathbf{e}_j)\\right]\\biggr|_{\\alpha = 0} &= \\argmax\\limits_{j=1,\\dots,m} \\left[-\\frac{\\partial}{\\partial \\alpha} \\frac{1}{n}\\sum_{i=1}^n [y_i - h(\\mathbf{x}_i)({\\mathbf{\\lambda}} + \\alpha\\mathbf{e}_j)]^2\\right]\\biggr|_{\\alpha = 0} \\\\\n",
    "                                               &=\\argmax\\limits_{j=1,\\dots,m} \\left[-\\frac{2}{n}\\sum_{i=1}^n [y_i - h(\\mathbf{x}_i)\\mathbf{({\\lambda}} + \\alpha\\mathbf{e}_j)][-h(\\mathbf{x}_i) \\mathbf{e}_j]\\right]\\biggr|_{\\alpha = 0}\\\\\n",
    "                                               &=\\argmax\\limits_{j=1,\\dots,m} \\left[\\sum_{i=1}^n [y_i - h(\\mathbf{x}_i)\\mathbf{({\\lambda}} + \\alpha\\mathbf{e}_j)]h_j(\\mathbf{x}_i)\\right]\\biggr|_{\\alpha = 0} \\\\\n",
    "                                               &=\\argmax\\limits_{j=1,\\dots,m} \\left[\\sum_{i=1}^n [y_i - h(\\mathbf{x}_i)\\mathbf{{\\lambda}}]h_j(\\mathbf{x}_i)\\right]\\\\\n",
    "                                               &=\\argmax\\limits_{j=1,\\dots,m} \\left[\\sum_{i=1}^n y_ih_j(\\mathbf{x}_i) - h_j(\\mathbf{x}_i)h(\\mathbf{x}_i)\\mathbf{{\\lambda}}\\right] \\\\\n",
    "                                               &=\\argmax\\limits_{j=1,\\dots,m} \\left[\\sum_{i=1}^n y_ih_j(\\mathbf{x}_i) - \\sum_{i=1}^nh_j(\\mathbf{x}_i)h(\\mathbf{x}_i)\\mathbf{{\\lambda}}\\right]\n",
    "\\end{align} \n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "The RHS: \n",
    "<br>\n",
    "$\n",
    "\\begin{align}\n",
    "& \\ \\ \\ \\ \\ \\argmax\\limits_{j=1,\\dots, m} h_j^T(\\mathbf{X})(\\mathbf{y} - H(\\mathbf{X})\\mathbf{\\lambda})\\\\ &= \\argmax\\limits_{j=1,\\dots, m} h_j^T(\\mathbf{X})\\mathbf{y} -h_j^T(\\mathbf{X}) H(\\mathbf{X})\\mathbf{\\lambda} \\\\\n",
    "                                                                                            &=\\argmax\\limits_{j=1,\\dots, m}\\begin{bmatrix}h_j(\\mathbf{x}_1) \\cdots h_j(\\mathbf{x}_n)\\end{bmatrix} \\cdot  \\begin{bmatrix}y_1\\\\\\vdots\\\\ y_n\\end{bmatrix} - \\begin{bmatrix}h_j(\\mathbf{x}_1) \\cdots  h_j(\\mathbf{x}_n)\\end{bmatrix} \\cdot\\begin{bmatrix}\n",
    "h_1(\\mathbf{x}_1) & \\dots & h_m(\\mathbf{x}_1)\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "h_1(\\mathbf{x}_n) & \\dots & h_m(\\mathbf{x}_n)\n",
    "\\end{bmatrix} \\cdot {\\lambda} \\\\\n",
    "&=\\argmax\\limits_{j=1,\\dots, m}( h_j(\\mathbf{x}_1)y_1 + h_j(\\mathbf{x}_2)y_2+ \\cdots h_j(\\mathbf{x}_n)y_n )- \\left[h_j(\\mathbf{x}_1)h_1(\\mathbf{x}_1) + \\cdots +h_j(\\mathbf{x}_n)h_1(\\mathbf{x}_n)\\;, \\;\\;\\cdots \\;\\;, \\;h_j(\\mathbf{x}_1)h_m(\\mathbf{x}_1) + \\cdots +h_j(\\mathbf{x}_n)h_m(\\mathbf{x}_n)  \\right ] \\cdot \\lambda \\\\\n",
    "&=\\argmax\\limits_{j=1,\\dots, m}\\sum_{i=1}^n h_j(\\mathbf{x}_i)y_i - \\left[\\sum_{i=1}^n h_j(\\mathbf{x}_i)h_1(\\mathbf{x_i}) \\;, \\;\\;\\cdots \\;\\;, \\;\\sum_{i=1}^n h_j(\\mathbf{x}_i)h_m(\\mathbf{x_i})  \\right] \\cdot \\lambda\\\\\n",
    "&=\\argmax\\limits_{j=1,\\dots, m}\\sum_{i=1}^n h_j(\\mathbf{x}_i)y_i - \\sum_{i=1}^n h_j(\\mathbf{x}_i)\\left[h_1(\\mathbf{x_i}) \\;, \\;\\;\\cdots \\;\\;, \\;h_m(\\mathbf{x_i}) \\right] \\cdot \\lambda \\\\\n",
    "&=\\argmax\\limits_{j=1,\\dots, m}\\sum_{i=1}^n y_ih_j(\\mathbf{x}_i) - \\sum_{i=1}^n h_j(\\mathbf{x}_i)h(\\mathbf{x_i})\\lambda\n",
    "\\end{align}\n",
    "$\n",
    "<br>\n",
    "<br>\n",
    "Therefore, $\\argmax\\limits_{j=1,\\dots,m} \\left[-\\frac{\\partial}{\\partial \\alpha}L({\\mathbf{\\lambda}} + \\alpha\\mathbf{e}_j)\\right]\\biggr|_{\\alpha = 0} =\\argmax\\limits_{j=1,\\dots, m} h_j^T(\\mathbf{X})(\\mathbf{y} - H(\\mathbf{X})\\mathbf{\\lambda}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "<font size = 4.5>\n",
    "$\n",
    "\\frac{\\partial L(\\lambda + \\alpha\\mathbf{e}_j)}{\\partial \\alpha} \\; \\biggr|_{\\alpha^{(t)}} \n",
    "$\n",
    "</font>\n",
    "= 0, \n",
    "solve for $\\alpha^{(t)}$\n",
    "<br>\n",
    "<font size = 3.5>\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(\\lambda + \\alpha\\mathbf{e}_j^{(t)})}{\\partial \\alpha} \\; \\biggr|_{\\alpha^{(t)}} \n",
    "&= \\frac{2}{n}\\sum_{i=1}^n [y_i - h(\\mathbf{x}_i)\\mathbf{({\\lambda}} + \\alpha{\\mathbf{e}_j})](-h(\\mathbf{x}_i){\\mathbf{e}_j} )\\; \\biggr|_{\\alpha^{(t)}}  \\\\ \n",
    "&= \\frac{2}{n}\\sum_{i=1}^n [y_i - h(\\mathbf{x}_i)\\mathbf{({\\lambda}} + \\alpha^{(t)} {\\mathbf{e}_{j^{(t)}}})](-{h_{j^{(t)}}}(\\mathbf{x}_i)) \\\\\n",
    "&=\\frac{2}{n}\\sum_{i=1}^n \\left[-y_i{h_{j^{(t)}}}(\\mathbf{x}_i) + h(\\mathbf{x}_i)\\mathbf{{\\lambda}}{h_{j^{(t)}}}(\\mathbf{x}_i) + h(\\mathbf{x}_i)\\alpha^{(t)} {\\mathbf{e}_{j^{(t)}}}{h_{j^{(t)}}}(\\mathbf{x}_i)\\right] \\\\\n",
    "&=\\frac{2}{n}(-\\sum_{i=1}^n y_i{h_{j^{(t)}}}(\\mathbf{x}_i) - h(\\mathbf{x}_i)\\mathbf{{\\lambda}}{h_{j^{(t)}}}(\\mathbf{x}_i) + \\sum_{i=1}^n{h_{j^{(t)}}}(\\mathbf{x}_i)\\alpha^{(t)}{h_{j^{(t)}}}(\\mathbf{x}_i)) \\\\\n",
    "&=\\frac{2}{n}(-h_{j^{(t)}}^T(\\mathbf{X})(\\mathbf{y} - H(\\mathbf{X})\\mathbf{\\lambda}) + \\sum_{i=1}^n{h^2_{j^{(t)}}}(\\mathbf{x}_i)\\alpha^{(t)}) \\\\\n",
    "&= 0  \\\\\n",
    "\\end{align} \\\\\n",
    "\\sum_{i=1}^n{h^2_{j^{(t)}}}(\\mathbf{x}_i)\\alpha^{(t)} = h_{j^{(t)}}^T(\\mathbf{X})(\\mathbf{y} - H(\\mathbf{X})\\mathbf{\\lambda}) \\\\\n",
    "\\alpha^{(t)} = \\frac{h_{j^{(t)}}^T(\\mathbf{X})(\\mathbf{y} - H(\\mathbf{X})\\mathbf{\\lambda})}{\\sum_{i=1}^n{h^2_{j^{(t)}}}(\\mathbf{x}_i)}\n",
    "$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given training data $D = {(x_i, y_i)}^n_{i=1}$ and maximum number of iterations T\n",
    "<br>\n",
    "Initialize weights: $\\boldsymbol\\lambda_1 = \\mathbf{0}$\n",
    "<br>\n",
    "$\\textbf{For } t=1, ... , T \\textbf{ do:}$\n",
    "<br>\n",
    "&nbsp;&nbsp;    $j_t \\in \\argmax\\limits_{j=1,\\dots, m} h_j^T(\\mathbf{X})(\\mathbf{y} - H(\\mathbf{X})\\mathbf{\\lambda}) $\n",
    "<br>    \n",
    "&nbsp; &nbsp;  compute for the coefficient:\n",
    " <font size = 3.5>\n",
    " $ \\alpha^{(t)} = \\frac{h_{j^{(t)}}^T(\\mathbf{X})(\\mathbf{y} - H(\\mathbf{X})\\mathbf{\\lambda})}{\\sum_{i=1}^n{h^2_{j^{(t)}}}(\\mathbf{x}_i)} $\n",
    "<br>\n",
    "</font>\n",
    "<br>\n",
    "&nbsp; &nbsp; update the weights for classifier: $\\lambda_{t+1} = \\lambda_{t} + \\alpha \\mathbf{e}_{jt}$\n",
    " <br>\n",
    "<br>\n",
    "$\\textbf{end for}$ \n",
    "<br>\n",
    "output the \n",
    "<font size = 3.5>\n",
    "$\\lambda h(x_i) = \\sum_{t=1}^T\\frac{h_{j^{(t)}}^T(\\mathbf{X})(\\mathbf{y} - H(\\mathbf{X})\\mathbf{\\lambda})}{\\sum_{i=1}^n{h^2_{j^{(t)}}}(\\mathbf{x}_i)}h_{j^{(t)}}(x_i)\n",
    "$\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Comparing AdaBoost and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "training_data = pd.read_csv('house_votes_84_train.csv').values\n",
    "testing_data = pd.read_csv('house_votes_84_test.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "    def __init__(self, training_data , testing_data):\n",
    "        self.label = training_data.T[0]\n",
    "        self.data = training_data[:,1:] \n",
    "        self.M = np.zeros((self.data.shape[0], self.data.shape[1]))   #matrix of margin for a weak classifier \n",
    "        for i in range(self.data.shape[1]):\n",
    "            self.M[:, i] = self.label * self.data[:,i] \n",
    "        self.lambda_ = np.zeros(self.data.shape[1])\n",
    "        self.test_data = testing_data[:,1:]\n",
    "        self.test_label = testing_data.T[0]\n",
    "\n",
    "    def train_adaboost(self): \n",
    "        d= np.ones(self.data.shape[0])/self.data.shape[0]\n",
    "        pre_error = 0;\n",
    "        error = 1;\n",
    "        error_index = []\n",
    "        Flag = False \n",
    "        count =1\n",
    "        while (abs(error - pre_error )> 10**-5):\n",
    "            if Flag:\n",
    "                pre_error = error \n",
    "            error = 0\n",
    "            j = np.argmax(np.dot(d.T,self.M))\n",
    "            for i in range(len(self.M.T[j])):\n",
    "                if self.M.T[j][i] < 0:\n",
    "                    error += d[i]\n",
    "                    error_index.append(i)\n",
    "            for k in range(len(d)):\n",
    "                if k in error_index:\n",
    "                    d[k] = 1/2*(d[k]/error)\n",
    "                else:\n",
    "                    d[k] = 1/2*(d[k]/(1-error))\n",
    "            alpha = 1/2*np.log((1-error) / error)\n",
    "            self.lambda_[j]+=alpha\n",
    "            error_index.clear()\n",
    "            Flag = True \n",
    "        \n",
    "    def predict(self,features):\n",
    "        return np.sign(np.dot(self.lambda_, features))\n",
    "    \n",
    "    def accuracy(self):\n",
    "        sum = 0;\n",
    "        for x in range(self.test_data.shape[0]):\n",
    "            if self.predict(self.test_data[x]) * self.test_label[x] > 0:\n",
    "                sum += 1\n",
    "        print(sum/self.test_data.shape[0])\n",
    "        return sum/self.test_data.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from math import log\n",
    "import sys\n",
    "class LR:\n",
    "    def __init__(self, training_data):\n",
    "        self.label = training_data.T[0]\n",
    "        self.data = training_data[:,1:] \n",
    "        self.lambda_ = np.zeros(self.data.shape[1])\n",
    "    def loss_fun(self, Lambda):\n",
    "        sum = 0 \n",
    "        for i in range(self.data.shape[0]): \n",
    "            sum += log(1+np.exp(-self.label[i] * np.dot(Lambda.T,self.data[i])))  \n",
    "        return sum\n",
    "    def maximization(self):\n",
    "        self.lambda_ = minimize(self.loss_fun,self.lambda_, method='SLSQP', tol =1e-6).x\n",
    "    def logistic_function(self,features):\n",
    "        probability = np.exp(np.dot(self.lambda_.T,features))/(1+np.exp(np.dot(self.lambda_.T,features)))\n",
    "        return probability\n",
    "    def predict(self,features):\n",
    "        probability_1 = self.logistic_function(features)\n",
    "        if probability_1 < 0.5:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Notice: functions for calculating LR accuracy and making predictions for large dataset  They are not used for LR\n",
    "#impelementation!\n",
    "def LR_accuracy(real_label, predict_label):\n",
    "    misclassified_total = 0\n",
    "    for i in real_label*np.array(predict_label):\n",
    "        if i < 0:\n",
    "            misclassified_total+=1\n",
    "    print(1-misclassified_total/len(real_label))\n",
    "    return 1-misclassified_total/len(real_label)\n",
    "    \n",
    "def predict_dataset(dataset, model):\n",
    "    predicted_label = []\n",
    "    for i in range(dataset[:,1:].shape[0]):\n",
    "        label1 = model.predict(dataset[:,1:][i])\n",
    "        predicted_label.append(label1)\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy for adaboost is 0.9428571428571428\n",
      "The testing accuracy for adaboost is 0.98\n"
     ]
    }
   ],
   "source": [
    "### adaboost train on training data set; test on both training and testing set \n",
    "Adaboost_training_object = Adaboost(training_data, training_data)\n",
    "Adaboost_training_object.train_adaboost()\n",
    "print('The training accuracy for adaboost is ',end='')\n",
    "_ = Adaboost_training_object.accuracy()\n",
    "Adaboost_testing_object = Adaboost(training_data, testing_data)\n",
    "Adaboost_testing_object.train_adaboost()\n",
    "print('The testing accuracy for adaboost is ',end='')\n",
    "_ = Adaboost_testing_object.accuracy()\n",
    "ada_prediction_testset = predict_dataset(testing_data,Adaboost_testing_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy for LR is 0.9636363636363636\n",
      "The testing accuracy for LR is 0.94\n"
     ]
    }
   ],
   "source": [
    "# same thing on the LR \n",
    "Logistic_object = LR(training_data)\n",
    "Logistic_object.maximization()\n",
    "training_predict_label = predict_dataset(training_data, Logistic_object)\n",
    "print(\"The training accuracy for LR is \", end='')\n",
    "_=LR_accuracy(training_data.T[0], training_predict_label)\n",
    "testing_predict_label = predict_dataset(testing_data,Logistic_object)\n",
    "print(\"The testing accuracy for LR is \", end='')\n",
    "_=LR_accuracy(testing_data.T[0], testing_predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## ROC curves\n",
    "import sklearn.metrics \n",
    "import matplotlib.pyplot as plt\n",
    "fpr_ad,tpr_ad,_ = sklearn.metrics.roc_curve(testing_data.T[0], ada_prediction_testset)\n",
    "fpr_lr,tpr_lr,_ = sklearn.metrics.roc_curve(testing_data.T[0] , testing_predict_label)\n",
    "plt.plot(fpr_ad,tpr_ad,label = \"ROC for Adaboost\")\n",
    "plt.plot(fpr_lr,tpr_lr,label = \"ROC for Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression has higher training accuracy but we care more about the test accuracy which the adaboost is 4% accuracy higher than logistic regression. Therefore, given the test set, adaboost outperforms the logsitic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost for cross validation:\n",
      "Accuracy for fold1:   0.9487179487179487\n",
      "\n",
      "Accuracy for fold2:   0.9487179487179487\n",
      "\n",
      "Accuracy for fold3:   0.9743589743589743\n",
      "\n",
      "Accuracy for fold4:   0.9487179487179487\n",
      "\n",
      "Accuracy for fold5:   0.9743589743589743\n",
      "\n",
      "Accuracy for fold6:   0.9473684210526315\n",
      "\n",
      "Accuracy for fold7:   0.9473684210526315\n",
      "\n",
      "Accuracy for fold8:   0.9736842105263158\n",
      "\n",
      "Accuracy for fold9:   0.7894736842105263\n",
      "\n",
      "Accuracy for fold10:   0.9736842105263158\n",
      "\n",
      "The average accuracy for adaboost is 0.9426450742240216\n",
      "The stardard deviation for adaboost is 0.05533246287605921\n"
     ]
    }
   ],
   "source": [
    "### cross validation for adaboost\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from statistics import stdev, mean\n",
    "cv= KFold(n_splits=10)\n",
    "fold_number =1\n",
    "accuracy_list =[]\n",
    "print('AdaBoost for cross validation:')\n",
    "for train, test in cv.split(training_data):\n",
    "    Adaboost_object = Adaboost(training_data[train],training_data[test])\n",
    "    Adaboost_object.train_adaboost()\n",
    "    print('Accuracy for fold' + str(fold_number) + ':   ', end='')\n",
    "    accuracy = Adaboost_object.accuracy()\n",
    "    print('')\n",
    "    accuracy_list.append(accuracy)\n",
    "    fold_number +=1\n",
    "mean_Ada = mean(accuracy_list)\n",
    "std_Ada =stdev(accuracy_list)\n",
    "print('The average accuracy for adaboost is', mean_Ada)\n",
    "print('The stardard deviation for adaboost is', std_Ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logisitic Regression for cross validation:\n",
      "Accuracy for fold1:  0.9743589743589743\n",
      "\n",
      "Accuracy for fold2:  0.9743589743589743\n",
      "\n",
      "Accuracy for fold3:  0.9487179487179487\n",
      "\n",
      "Accuracy for fold4:  0.9487179487179487\n",
      "\n",
      "Accuracy for fold5:  0.9487179487179487\n",
      "\n",
      "Accuracy for fold6:  0.9473684210526316\n",
      "\n",
      "Accuracy for fold7:  0.9736842105263158\n",
      "\n",
      "Accuracy for fold8:  0.8947368421052632\n",
      "\n",
      "Accuracy for fold9:  0.8421052631578947\n",
      "\n",
      "Accuracy for fold10:  0.9473684210526316\n",
      "\n",
      "The average accuracy for logistic regression is 0.9400134952766531\n",
      "The stardard deviation for logistic regression is 0.041452021216918246\n"
     ]
    }
   ],
   "source": [
    "### cross validation  for LR\n",
    "fold_number =1\n",
    "accuracy_list2=[]\n",
    "print('Logisitic Regression for cross validation:')\n",
    "for train, test in cv.split(training_data):\n",
    "    LR_classifier = LR(training_data[train])\n",
    "    LR_classifier.maximization()\n",
    "    LR_predicted_labels = predict_dataset(training_data[test], LR_classifier) \n",
    "    print('Accuracy for fold'+ str(fold_number) + ':  ', end='') \n",
    "    accuracy = LR_accuracy(training_data[test].T[0], LR_predicted_labels)\n",
    "    print('')\n",
    "    accuracy_list2.append(accuracy)\n",
    "    fold_number +=1\n",
    "\n",
    "mean_LR = mean(accuracy_list2)\n",
    "std_LR =stdev(accuracy_list2)\n",
    "print('The average accuracy for logistic regression is', mean_LR)\n",
    "print('The stardard deviation for logistic regression is', std_LR)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cross validation, we can see the average accuracy for both alogrithms are very close, adaboost slightly outperforms logisitic regression on the accuracy, something around 0.2% but LR has stardard deviation which is 0.5% lower than adaboost does. Therefore, the performances for both algorithm are almost the same for this real-world data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Testing accuracy with permuted features for Adaboost\n",
      "bill_1: 0.98\n",
      "bill_2: 0.98\n",
      "bill_3: 0.98\n",
      "bill_4: 0.62\n",
      "bill_5: 0.96\n",
      "bill_6: 0.98\n",
      "bill_7: 0.98\n",
      "bill_8: 0.98\n",
      "bill_9: 0.98\n",
      "bill_10: 0.98\n",
      "bill_11: 0.98\n",
      "bill_12: 0.96\n",
      "bill_13: 0.98\n",
      "bill_14: 0.98\n",
      "bill_15: 0.98\n",
      "bill_16: 0.98\n"
     ]
    }
   ],
   "source": [
    "# variable importance. randomly permute a feature on the test set and look into the accuracy change\n",
    "# adaboost \n",
    "testing_dataframe =  pd.read_csv('house_votes_84_test.csv')\n",
    "original_dataframe = testing_dataframe.copy(deep=True)\n",
    "column_names =list(testing_dataframe)\n",
    "print('The Testing accuracy with permuted features for Adaboost')\n",
    "for name in column_names[1:]:\n",
    "    testing_dataframe[name] = np.random.permutation(testing_dataframe[name])\n",
    "    test_new_data = testing_dataframe.values\n",
    "    Adaboost_permuted_object = Adaboost(training_data, test_new_data)\n",
    "    print(name + ': ',end='')\n",
    "    Adaboost_permuted_object.train_adaboost()\n",
    "    _ = Adaboost_permuted_object.accuracy()\n",
    "    testing_dataframe=original_dataframe.copy(deep=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy with permuted features for Logistic Regression\n",
      "bill_1: 0.96\n",
      "bill_2: 0.94\n",
      "bill_3: 0.9\n",
      "bill_4: 0.62\n",
      "bill_5: 0.94\n",
      "bill_6: 0.98\n",
      "bill_7: 0.96\n",
      "bill_8: 0.92\n",
      "bill_9: 0.94\n",
      "bill_10: 0.96\n",
      "bill_11: 0.94\n",
      "bill_12: 0.96\n",
      "bill_13: 0.96\n",
      "bill_14: 0.96\n",
      "bill_15: 0.94\n",
      "bill_16: 0.94\n"
     ]
    }
   ],
   "source": [
    "# same thing for the LR \n",
    "testing_dataframe =  pd.read_csv('house_votes_84_test.csv')\n",
    "original_dataframe = testing_dataframe.copy(deep=True)\n",
    "column_names =list(testing_dataframe)\n",
    "print(\"The testing accuracy with permuted features for Logistic Regression\")\n",
    "for name in column_names[1:]:\n",
    "    testing_dataframe[name] = np.random.permutation(testing_dataframe[name])\n",
    "    test_new_data = testing_dataframe.values\n",
    "    Logistic_model = LR(training_data)\n",
    "    Logistic_model.maximization()\n",
    "    testing_predict_label = predict_dataset(test_new_data,Logistic_model)\n",
    "    print(name + ': ', end='')\n",
    "    LR_accuracy(test_new_data.T[0], testing_predict_label)\n",
    "    testing_dataframe=original_dataframe.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the variable importance method to randomly permute the values for each features on the test data set, more specifically each bill, and find out the accuracy of both the adaboost and the logistic regression testing on the permuted-features data set. Both learning algorithms show that the fourth bill is extremely crucial that it is almost dominiating the importances of all the other bills because no matter which feature except bill_4 is permuted, the overall accuracy is around 95% (or higher) for both algorithms. However, once we shuffled the values for bill_4, adaboost acted worse than a weak classifier (below 0.5), and the performance of Logisitic Regression falls dramatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
